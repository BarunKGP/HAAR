{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "functools.partial(<class 'torch.optim.lr_scheduler.CosineAnnealingLR'>, T_max=10, eta_min=0.0001)\n"
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import LRScheduler\n",
    "import sys\n",
    "from pprint import PrettyPrinter\n",
    "pp = PrettyPrinter(indent=2)\n",
    "from functools import partial\n",
    "\n",
    "opt = getattr(sys.modules['torch.optim.lr_scheduler'], 'CosineAnnealingLR')\n",
    "# print(opt)\n",
    "kwarg = {'T_max': 10, 'eta_min': 1e-4}\n",
    "lrs = partial(opt, **kwarg)\n",
    "print(lrs)\n",
    "# pp.pprint(list(sys.modules.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = [1, 2, 3]\n",
    "y = [1.000001, 2.0000001, 3]\n",
    "z = np.allclose(x, y, atol=1e-4)\n",
    "print(z)\n",
    "print(all(t > 1 for t in y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1230],\n",
      "        [0.1420]])\n",
      "tensor(0.2650)\n",
      "0.26500001549720764\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.zeros(2, 1)\n",
    "a[0] = 0.123\n",
    "a[1] = 0.142\n",
    "print(a)\n",
    "print(a.sum())\n",
    "print(a.sum().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from omegaconf import OmegaConf\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 10 3\n",
      "1 16 3\n",
      "2 32 5\n"
     ]
    }
   ],
   "source": [
    "ks = (3, 3, 5)\n",
    "nc = (10, 16, 32)\n",
    "for i, (a, b) in enumerate(zip(nc, ks)):\n",
    "    print(i, a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, -12, 2, 34]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ks = [1, 2, 34]\n",
    "ks.insert(1, -12)\n",
    "ks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3484, 0.7877, 0.9607]])\n",
      "tensor([[[0.3484, 0.7877, 0.9607]],\n",
      "\n",
      "        [[0.3484, 0.7877, 0.9607]],\n",
      "\n",
      "        [[0.3484, 0.7877, 0.9607]],\n",
      "\n",
      "        [[0.3484, 0.7877, 0.9607]]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(1, 3)\n",
    "print(x)\n",
    "\n",
    "y = torch.unsqueeze(x, 0).expand(4, -1, -1)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "expand(torch.FloatTensor{[4, 1, 10]}, size=[32]): the number of sizes provided (1) must be greater or equal to the number of dimensions in the tensor (3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/coc/scratch/bdas31/EPIC-Kitchens_100/HAAR/src/huggingface.ipynb Cell 3\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bzatopek.cc.gatech.edu/coc/scratch/bdas31/EPIC-Kitchens_100/HAAR/src/huggingface.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrand(\u001b[39m4\u001b[39m, \u001b[39m10\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bzatopek.cc.gatech.edu/coc/scratch/bdas31/EPIC-Kitchens_100/HAAR/src/huggingface.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m y \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49munsqueeze(x, \u001b[39m1\u001b[39;49m)\u001b[39m.\u001b[39;49mexpand(\u001b[39m32\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bzatopek.cc.gatech.edu/coc/scratch/bdas31/EPIC-Kitchens_100/HAAR/src/huggingface.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(x\u001b[39m.\u001b[39mshape)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bzatopek.cc.gatech.edu/coc/scratch/bdas31/EPIC-Kitchens_100/HAAR/src/huggingface.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(y\u001b[39m.\u001b[39mshape)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expand(torch.FloatTensor{[4, 1, 10]}, size=[32]): the number of sizes provided (1) must be greater or equal to the number of dimensions in the tensor (3)"
     ]
    }
   ],
   "source": [
    "x = torch.rand(4, 10)\n",
    "y = torch.unsqueeze(x, 1).expand(32)\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 28743 entries, 0 to 28742\n",
      "Data columns (total 10 columns):\n",
      " #   Column               Non-Null Count  Dtype \n",
      "---  ------               --------------  ----- \n",
      " 0   Unnamed: 0           28743 non-null  int64 \n",
      " 1   video_id             28743 non-null  object\n",
      " 2   root_dir             28743 non-null  object\n",
      " 3   narration_id         28743 non-null  object\n",
      " 4   narration            28743 non-null  object\n",
      " 5   narration_timestamp  28743 non-null  object\n",
      " 6   start_frame          28743 non-null  int64 \n",
      " 7   end_frame            28743 non-null  int64 \n",
      " 8   verb_class           28743 non-null  int64 \n",
      " 9   noun_class           28743 non-null  int64 \n",
      "dtypes: int64(5), object(5)\n",
      "memory usage: 2.2+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>video_id</th>\n",
       "      <th>root_dir</th>\n",
       "      <th>narration_id</th>\n",
       "      <th>narration</th>\n",
       "      <th>narration_timestamp</th>\n",
       "      <th>start_frame</th>\n",
       "      <th>end_frame</th>\n",
       "      <th>verb_class</th>\n",
       "      <th>noun_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>P01_103</td>\n",
       "      <td>../2g1n6qdydwa9u22shpxqzp0t8m/P01</td>\n",
       "      <td>P01_103_0</td>\n",
       "      <td>close door</td>\n",
       "      <td>00:00:02.103</td>\n",
       "      <td>6</td>\n",
       "      <td>381</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>P01_103</td>\n",
       "      <td>../2g1n6qdydwa9u22shpxqzp0t8m/P01</td>\n",
       "      <td>P01_103_1</td>\n",
       "      <td>pick up plate</td>\n",
       "      <td>00:00:07.615</td>\n",
       "      <td>281</td>\n",
       "      <td>493</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>P01_103</td>\n",
       "      <td>../2g1n6qdydwa9u22shpxqzp0t8m/P01</td>\n",
       "      <td>P01_103_2</td>\n",
       "      <td>open cupboard</td>\n",
       "      <td>00:00:09.850</td>\n",
       "      <td>393</td>\n",
       "      <td>531</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>P01_103</td>\n",
       "      <td>../2g1n6qdydwa9u22shpxqzp0t8m/P01</td>\n",
       "      <td>P01_103_3</td>\n",
       "      <td>put plate into cupboard</td>\n",
       "      <td>00:00:10.600</td>\n",
       "      <td>431</td>\n",
       "      <td>593</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>P01_103</td>\n",
       "      <td>../2g1n6qdydwa9u22shpxqzp0t8m/P01</td>\n",
       "      <td>P01_103_4</td>\n",
       "      <td>close cupboard</td>\n",
       "      <td>00:00:11.850</td>\n",
       "      <td>493</td>\n",
       "      <td>668</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 video_id                           root_dir narration_id  \\\n",
       "0           0  P01_103  ../2g1n6qdydwa9u22shpxqzp0t8m/P01    P01_103_0   \n",
       "1           1  P01_103  ../2g1n6qdydwa9u22shpxqzp0t8m/P01    P01_103_1   \n",
       "2           2  P01_103  ../2g1n6qdydwa9u22shpxqzp0t8m/P01    P01_103_2   \n",
       "3           3  P01_103  ../2g1n6qdydwa9u22shpxqzp0t8m/P01    P01_103_3   \n",
       "4           4  P01_103  ../2g1n6qdydwa9u22shpxqzp0t8m/P01    P01_103_4   \n",
       "\n",
       "                 narration narration_timestamp  start_frame  end_frame  \\\n",
       "0               close door        00:00:02.103            6        381   \n",
       "1            pick up plate        00:00:07.615          281        493   \n",
       "2            open cupboard        00:00:09.850          393        531   \n",
       "3  put plate into cupboard        00:00:10.600          431        593   \n",
       "4           close cupboard        00:00:11.850          493        668   \n",
       "\n",
       "   verb_class  noun_class  \n",
       "0           4           3  \n",
       "1           0           2  \n",
       "2           3           3  \n",
       "3           5           2  \n",
       "4           4           3  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "with open('../data/train100_mod.pkl', 'rb') as f:\n",
    "    df = pickle.load(f)\n",
    "\n",
    "df.info()\n",
    "display(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>end_frame</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P01_103</td>\n",
       "      <td>8906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P01_105</td>\n",
       "      <td>106023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P01_106</td>\n",
       "      <td>36174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P01_107</td>\n",
       "      <td>5977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>P01_108</td>\n",
       "      <td>7000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>P35_108</td>\n",
       "      <td>150759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>P35_109</td>\n",
       "      <td>56479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>P37_101</td>\n",
       "      <td>84078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>P37_102</td>\n",
       "      <td>34136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>P37_103</td>\n",
       "      <td>19761</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>149 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    video_id  end_frame\n",
       "0    P01_103       8906\n",
       "1    P01_105     106023\n",
       "2    P01_106      36174\n",
       "3    P01_107       5977\n",
       "4    P01_108       7000\n",
       "..       ...        ...\n",
       "144  P35_108     150759\n",
       "145  P35_109      56479\n",
       "146  P37_101      84078\n",
       "147  P37_102      34136\n",
       "148  P37_103      19761\n",
       "\n",
       "[149 rows x 2 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('video_id')['end_frame'].max().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3708.12\n",
      "4096\n"
     ]
    }
   ],
   "source": [
    "print(185406/50)\n",
    "print(2**12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn(bn, in_feat_dim, n_conv, k_sizes, dropout=0.1):\n",
    "    if len(bn) > 0:\n",
    "        layers = [\n",
    "            [\n",
    "                nn.Conv1d(ni, nc, kf, bias=False if i in bn else True), \n",
    "                nn.ReLU(), \n",
    "                nn.Dropout(dropout),\n",
    "            ] \n",
    "            for i, (ni, nc, kf) in enumerate(zip(in_feat_dim, n_conv, k_sizes))\n",
    "        ]\n",
    "        for i in bn:\n",
    "            layers[i].insert(2, nn.BatchNorm2d(n_conv[i]))\n",
    "\n",
    "            \n",
    "    else:\n",
    "        layers = [\n",
    "            [nn.Conv1d(ni, nc, kf, bias=True), nn.ReLU(), nn.Dropout(dropout),] \n",
    "            for ni, nc, kf in zip(in_feat_dim, n_conv, k_sizes)\n",
    "        ]\n",
    "\n",
    "    linear_layer = nn.Sequential(*layers)\n",
    "    print(linear_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list is not a Module subclass",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/coc/scratch/bdas31/EPIC-Kitchens_100/HAAR/src/huggingface.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bzatopek.cc.gatech.edu/coc/scratch/bdas31/EPIC-Kitchens_100/HAAR/src/huggingface.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m fn([\u001b[39m1\u001b[39;49m, \u001b[39m3\u001b[39;49m], [\u001b[39m1\u001b[39;49m, \u001b[39m8\u001b[39;49m, \u001b[39m32\u001b[39;49m, \u001b[39m16\u001b[39;49m], [\u001b[39m8\u001b[39;49m, \u001b[39m32\u001b[39;49m, \u001b[39m16\u001b[39;49m, \u001b[39m10\u001b[39;49m], [\u001b[39m3\u001b[39;49m, \u001b[39m5\u001b[39;49m, \u001b[39m3\u001b[39;49m, \u001b[39m3\u001b[39;49m])\n",
      "\u001b[1;32m/coc/scratch/bdas31/EPIC-Kitchens_100/HAAR/src/huggingface.ipynb Cell 10\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bzatopek.cc.gatech.edu/coc/scratch/bdas31/EPIC-Kitchens_100/HAAR/src/huggingface.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bzatopek.cc.gatech.edu/coc/scratch/bdas31/EPIC-Kitchens_100/HAAR/src/huggingface.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m     layers \u001b[39m=\u001b[39m [\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bzatopek.cc.gatech.edu/coc/scratch/bdas31/EPIC-Kitchens_100/HAAR/src/huggingface.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m         [nn\u001b[39m.\u001b[39mConv1d(ni, nc, kf, bias\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m), nn\u001b[39m.\u001b[39mReLU(), nn\u001b[39m.\u001b[39mDropout(dropout),] \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bzatopek.cc.gatech.edu/coc/scratch/bdas31/EPIC-Kitchens_100/HAAR/src/huggingface.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m         \u001b[39mfor\u001b[39;00m ni, nc, kf \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(in_feat_dim, n_conv, k_sizes)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bzatopek.cc.gatech.edu/coc/scratch/bdas31/EPIC-Kitchens_100/HAAR/src/huggingface.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m     ]\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bzatopek.cc.gatech.edu/coc/scratch/bdas31/EPIC-Kitchens_100/HAAR/src/huggingface.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m linear_layer \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39;49mSequential(\u001b[39m*\u001b[39;49mlayers)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bzatopek.cc.gatech.edu/coc/scratch/bdas31/EPIC-Kitchens_100/HAAR/src/huggingface.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mprint\u001b[39m(linear_layer)\n",
      "File \u001b[0;32m/coc/pcba1/bdas31/Miniconda3/envs/haar-2/lib/python3.10/site-packages/torch/nn/modules/container.py:104\u001b[0m, in \u001b[0;36mSequential.__init__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    103\u001b[0m     \u001b[39mfor\u001b[39;00m idx, module \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(args):\n\u001b[0;32m--> 104\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_module(\u001b[39mstr\u001b[39;49m(idx), module)\n",
      "File \u001b[0;32m/coc/pcba1/bdas31/Miniconda3/envs/haar-2/lib/python3.10/site-packages/torch/nn/modules/module.py:610\u001b[0m, in \u001b[0;36mModule.add_module\u001b[0;34m(self, name, module)\u001b[0m\n\u001b[1;32m    600\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"Adds a child module to the current module.\u001b[39;00m\n\u001b[1;32m    601\u001b[0m \n\u001b[1;32m    602\u001b[0m \u001b[39mThe module can be accessed as an attribute using the given name.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    607\u001b[0m \u001b[39m    module (Module): child module to be added to the module.\u001b[39;00m\n\u001b[1;32m    608\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    609\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(module, Module) \u001b[39mand\u001b[39;00m module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 610\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mtorch\u001b[39m.\u001b[39mtypename(module)\u001b[39m}\u001b[39;00m\u001b[39m is not a Module subclass\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    611\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(name, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    612\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmodule name should be a string. Got \u001b[39m\u001b[39m{\u001b[39;00mtorch\u001b[39m.\u001b[39mtypename(name)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: list is not a Module subclass"
     ]
    }
   ],
   "source": [
    "fn([1, 3], [1, 8, 32, 16], [8, 32, 16, 10], [3, 5, 3, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = OmegaConf.load('../configs/pilot_config.yaml')\n",
    "# print(OmegaConf.to_yaml(config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/coc/scratch/bdas31/EPIC-Kitchens_100/HAAR/src',\n",
      " '/coc/pcba1/bdas31/Miniconda3/envs/haar-2/lib/python310.zip',\n",
      " '/coc/pcba1/bdas31/Miniconda3/envs/haar-2/lib/python3.10',\n",
      " '/coc/pcba1/bdas31/Miniconda3/envs/haar-2/lib/python3.10/lib-dynload',\n",
      " '',\n",
      " '/coc/pcba1/bdas31/Miniconda3/envs/haar-2/lib/python3.10/site-packages',\n",
      " '/coc/pcba1/bdas31/Miniconda3/envs/haar-2/lib/python3.10/site-packages/PyQt5_sip-12.11.0-py3.10-linux-x86_64.egg',\n",
      " '/coc/pcba1/bdas31/Miniconda3/envs/haar-2/lib/python3.10/site-packages/huggingface_hub-0.16.4-py3.8.egg',\n",
      " '/coc/pcba1/bdas31/Miniconda3/envs/haar-2/lib/python3.10/site-packages/sacremoses-0.0.43-py3.8.egg']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pprint import PrettyPrinter\n",
    "\n",
    "pp = PrettyPrinter()\n",
    "pp.pprint(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/coc/scratch/bdas31/EPIC-Kitchens_100/HAAR/src/scratch',\n",
      " '/coc/pcba1/bdas31/Miniconda3/envs/haar-2/lib/python310.zip',\n",
      " '/coc/pcba1/bdas31/Miniconda3/envs/haar-2/lib/python3.10',\n",
      " '/coc/pcba1/bdas31/Miniconda3/envs/haar-2/lib/python3.10/lib-dynload',\n",
      " '',\n",
      " '/coc/pcba1/bdas31/Miniconda3/envs/haar-2/lib/python3.10/site-packages',\n",
      " '/coc/pcba1/bdas31/Miniconda3/envs/haar-2/lib/python3.10/site-packages/PyQt5_sip-12.11.0-py3.10-linux-x86_64.egg',\n",
      " '/coc/pcba1/bdas31/Miniconda3/envs/haar-2/lib/python3.10/site-packages/huggingface_hub-0.16.4-py3.8.egg',\n",
      " '/coc/pcba1/bdas31/Miniconda3/envs/haar-2/lib/python3.10/site-packages/sacremoses-0.0.43-py3.8.egg',\n",
      " '/coc/scratch/bdas31/EPIC-Kitchens_100/HAAR/',\n",
      " '/coc/scratch/bdas31/EPIC-Kitchens_100/HAAR/src',\n",
      " '/coc/scratch/bdas31/EPIC-Kitchens_100/HAAR/src/datasets']\n"
     ]
    }
   ],
   "source": [
    "sys.path.append('/coc/scratch/bdas31/EPIC-Kitchens_100/HAAR/src/datasets')\n",
    "pp.pprint(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from systems.data_module import EpicActionRecognitionDataModule\n",
    "\n",
    "\n",
    "data_module = EpicActionRecognitionDataModule(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.utils.data.dataloader.DataLoader'>\n"
     ]
    }
   ],
   "source": [
    "train_loader = data_module.test_dataloader()\n",
    "print(type(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-25 20:22:10,521 [MainThread  ] [WARNI]  model.pretrained was set to 'None' but you also specified to load weights from {'rgb': '/coc/scratch/bdas31/EPIC-Kitchens_100/C1-Action-Recognition-TSN-TRN-TSM/pretrained_models/tsm_rgb.ckpt', 'flow': '/coc/scratch/bdas31/EPIC-Kitchens_100/C1-Action-Recognition-TSN-TRN-TSM/pretrained_models/tsm_flow.ckpt'}.The latter will take precedence.\n",
      "2023-09-25 20:22:11,028 [MainThread  ] [WARNI]  model.pretrained was set to 'None' but you also specified to load weights from {'rgb': '/coc/scratch/bdas31/EPIC-Kitchens_100/C1-Action-Recognition-TSN-TRN-TSM/pretrained_models/tsm_rgb.ckpt', 'flow': '/coc/scratch/bdas31/EPIC-Kitchens_100/C1-Action-Recognition-TSN-TRN-TSM/pretrained_models/tsm_flow.ckpt'}.The latter will take precedence.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m x \u001b[39m=\u001b[39m [x \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m4\u001b[39m)]\n\u001b[1;32m     11\u001b[0m model \u001b[39m=\u001b[39m EmbeddingModel(config, \u001b[39m0.1\u001b[39m, device)\n\u001b[0;32m---> 12\u001b[0m y \u001b[39m=\u001b[39m model(x, permute_dims\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m     13\u001b[0m \u001b[39mprint\u001b[39m(y\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[0;32m/coc/pcba1/bdas31/Miniconda3/envs/haar-2/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/coc/pcba1/bdas31/Miniconda3/envs/haar-2/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/coc/scratch/bdas31/EPIC-Kitchens_100/HAAR/src/models/models.py:98\u001b[0m, in \u001b[0;36mEmbeddingModel.forward\u001b[0;34m(self, x, permute_dims, fp16)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, permute_dims\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, fp16\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m---> 98\u001b[0m     \u001b[39mprint\u001b[39m(x\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     99\u001b[0m     (rgb, metadata), (flow, _) \u001b[39m=\u001b[39m x\n\u001b[1;32m    100\u001b[0m     narration \u001b[39m=\u001b[39m metadata[\u001b[39m'\u001b[39m\u001b[39mnarration\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "import models\n",
    "import importlib\n",
    "# importlib.reload(models)\n",
    "from models.models import EmbeddingModel\n",
    "\n",
    "device = 'cuda:0'\n",
    "x = torch.rand(4, 32, 3, 224, 224, device=device)\n",
    "x = [x for _ in range(4)]\n",
    "model = EmbeddingModel(config, 0.1, device)\n",
    "y = model(x, permute_dims=False)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-25 20:11:31,402 [MainThread  ] [WARNI]  model.pretrained was set to 'None' but you also specified to load weights from {'rgb': '/coc/scratch/bdas31/EPIC-Kitchens_100/C1-Action-Recognition-TSN-TRN-TSM/pretrained_models/tsm_rgb.ckpt', 'flow': '/coc/scratch/bdas31/EPIC-Kitchens_100/C1-Action-Recognition-TSN-TRN-TSM/pretrained_models/tsm_flow.ckpt'}.The latter will take precedence.\n",
      "2023-09-25 20:11:31,778 [MainThread  ] [WARNI]  model.pretrained was set to 'None' but you also specified to load weights from {'rgb': '/coc/scratch/bdas31/EPIC-Kitchens_100/C1-Action-Recognition-TSN-TRN-TSM/pretrained_models/tsm_rgb.ckpt', 'flow': '/coc/scratch/bdas31/EPIC-Kitchens_100/C1-Action-Recognition-TSN-TRN-TSM/pretrained_models/tsm_flow.ckpt'}.The latter will take precedence.\n",
      "torch.Size([4, 32, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "from models.models import EmbeddingModel\n",
    "\n",
    "\n",
    "model = EmbeddingModel(config, 0.1, 'cuda:0')\n",
    "for x in train_loader:\n",
    "    # x = model(x, permute_dims=False)\n",
    "    # pp.pprint(metadata)\n",
    "    print(x[0][0].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "y = torch.mean(x, dim=1)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'datasets.epic_dataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m sys\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mappend(sys\u001b[39m.\u001b[39mpath[\u001b[39m0\u001b[39m] \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/..\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m AttentionModel\n\u001b[0;32m---> 11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msystems\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata_module\u001b[39;00m \u001b[39mimport\u001b[39;00m EpicActionRecognitionDataModule\n\u001b[1;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msystems\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mrecognition_module\u001b[39;00m \u001b[39mimport\u001b[39;00m EpicActionRecognitionModule\n\u001b[1;32m     13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtrain\u001b[39;00m \u001b[39mimport\u001b[39;00m create_snapshot_paths\n",
      "File \u001b[0;32m/coc/scratch/bdas31/EPIC-Kitchens_100/HAAR/src/scratch/../systems/data_module.py:6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m Union\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39momegaconf\u001b[39;00m \u001b[39mimport\u001b[39;00m DictConfig\n\u001b[0;32m----> 6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdatasets\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mepic_dataset\u001b[39;00m \u001b[39mimport\u001b[39;00m EpicVideoDataset, EpicVideoFlowDataset\n\u001b[1;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdatasets\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtsn_dataset\u001b[39;00m \u001b[39mimport\u001b[39;00m TsnDataset\n\u001b[1;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdatasets\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mhaar_dataset\u001b[39;00m \u001b[39mimport\u001b[39;00m HaarDataset\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'datasets.epic_dataset'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from pathlib import Path\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from omegaconf import OmegaConf, DictConfig\n",
    "import re\n",
    "\n",
    "import sys\n",
    "sys.path.append(sys.path[0] + \"/..\")\n",
    "from models.models import AttentionModel\n",
    "from systems.data_module import EpicActionRecognitionDataModule\n",
    "from systems.recognition_module import EpicActionRecognitionModule\n",
    "from train import create_snapshot_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = OmegaConf.load('../../configs/pilot_config.yaml')\n",
    "# device = torch.device('cuda:0')\n",
    "data_module = EpicActionRecognitionDataModule(cfg)\n",
    "verb_path, noun_path = create_snapshot_paths(Path(cfg.model.save_path))\n",
    "system = EpicActionRecognitionModule(cfg)\n",
    "# system.load_models_to_device(verb=True)\n",
    "# model = AttentionModel(system.verb_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from models.attention import Transformer\n",
    "\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(device)\n",
    "\n",
    "# x = torch.tensor([[1, 5, 6, 4, 3, 3, 5, 2, 0], [1, 1, 7, 3, 4, 5, 6, 7, 2]]).to(\n",
    "#     device\n",
    "# )\n",
    "# trg = torch.tensor([[1, 7, 4, 3, 5, 3, 2, 0], [1, 5, 6, 2, 4, 7, 6, 2]]).to(device)\n",
    "\n",
    "# src_pad_idx = 0\n",
    "# trg_pad_idx = 0\n",
    "# src_vocab_size = 8\n",
    "# trg_vocab_size = 8\n",
    "# model = Transformer(src_vocab_size, trg_vocab_size, src_pad_idx, trg_pad_idx, device=device).to(\n",
    "#     device\n",
    "# )\n",
    "# # out = model(x, trg[:, :-1])\n",
    "# # print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from constants import NUM_VERBS, WORD_EMBEDDING_SIZE\n",
    "from models.models import EmbeddingModel\n",
    "\n",
    "\n",
    "class HaarModel(nn.Module):\n",
    "    def __init__(self, verb_map, noun_map, dropout, device):\n",
    "        super().__init__()\n",
    "        self.verb_map = verb_map\n",
    "        self.noun_map = noun_map\n",
    "\n",
    "        self.device = device\n",
    "        self.feature_model = EmbeddingModel(cfg, dropout, device)\n",
    "        # self.transformer = nn.Transformer(\n",
    "        #     d_model=384, \n",
    "        #     nhead=4, \n",
    "        #     batch_first=True,\n",
    "        #     dropout=dropout, \n",
    "        #     device=device,\n",
    "        # )\n",
    "        self.transformer = Transformer(\n",
    "            src_vocab_size=NUM_VERBS,\n",
    "            trg_vocab_size=NUM_VERBS,\n",
    "            src_pad_idx=None,\n",
    "            embed_size=384,\n",
    "            dropout=0.1,\n",
    "            max_length=6,\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "        self.fc_out = nn.Linear(100*WORD_EMBEDDING_SIZE, NUM_VERBS)\n",
    "        \n",
    "    def _get_target_mask(self, feats):\n",
    "        trg_len = feats.shape[1]\n",
    "        trg_mask = torch.tril(torch.ones(trg_len, trg_len))\n",
    "        return trg_mask.to(self.device)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        N = 6 #! do not hardcode\n",
    "        feats = self.feature_model(x, permute_dims=False)\n",
    "        feats = self.transformer(feats, feats)\n",
    "        # target_mask = self._get_target_mask(feats)\n",
    "        # feats = self.transformer(feats, feats, tgt_mask=target_mask).reshape((N, -1))\n",
    "        # print(f'feats shape = {feats.shape}')\n",
    "        return self.fc_out(feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/coc/pcba1/bdas31/Miniconda3/envs/haar/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/coc/pcba1/bdas31/Miniconda3/envs/haar/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-09 18:10:12,450 [MainThread  ] [WARNI]  model.pretrained was set to 'None' but you also specified to load weights from {'rgb': '/coc/scratch/bdas31/EPIC-Kitchens_100/C1-Action-Recognition-TSN-TRN-TSM/pretrained_models/tsm_rgb.ckpt', 'flow': '/coc/scratch/bdas31/EPIC-Kitchens_100/C1-Action-Recognition-TSN-TRN-TSM/pretrained_models/tsm_flow.ckpt'}.The latter will take precedence.\n",
      "2023-08-09 18:10:12,948 [MainThread  ] [WARNI]  model.pretrained was set to 'None' but you also specified to load weights from {'rgb': '/coc/scratch/bdas31/EPIC-Kitchens_100/C1-Action-Recognition-TSN-TRN-TSM/pretrained_models/tsm_rgb.ckpt', 'flow': '/coc/scratch/bdas31/EPIC-Kitchens_100/C1-Action-Recognition-TSN-TRN-TSM/pretrained_models/tsm_flow.ckpt'}.The latter will take precedence.\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "haarModel = HaarModel(system.verb_map, system.noun_map, 0.1, device)\n",
    "training_dataset = data_module.train_dataloader()\n",
    "loss_fn = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "optimizer = AdamW(haarModel.parameters(), lr=cfg.learning.lr, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80855954\n"
     ]
    }
   ],
   "source": [
    "print(sum(p.numel() for p in haarModel.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trg shape = torch.Size([6, 100, 384])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mfor\u001b[39;00m i, batch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(training_dataset):\n\u001b[1;32m      9\u001b[0m     labels \u001b[39m=\u001b[39m batch[\u001b[39m0\u001b[39m][\u001b[39m1\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mverb_class\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m---> 10\u001b[0m     preds \u001b[39m=\u001b[39m haarModel(batch)\u001b[39m.\u001b[39mcpu()\n\u001b[1;32m     11\u001b[0m     loss \u001b[39m=\u001b[39m loss_fn(preds, labels)\n\u001b[1;32m     12\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m/coc/pcba1/bdas31/Miniconda3/envs/haar/lib/python3.8/site-packages/torch/nn/modules/module.py:1502\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1500\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1501\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1502\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/coc/pcba1/bdas31/Miniconda3/envs/haar/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1506\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1507\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1508\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1509\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1510\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1512\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[4], line 40\u001b[0m, in \u001b[0;36mHaarModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     38\u001b[0m N \u001b[39m=\u001b[39m \u001b[39m6\u001b[39m \u001b[39m#! do not hardcode\u001b[39;00m\n\u001b[1;32m     39\u001b[0m feats \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeature_model(x, permute_dims\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m---> 40\u001b[0m feats \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(feats, feats)\n\u001b[1;32m     41\u001b[0m \u001b[39m# target_mask = self._get_target_mask(feats)\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[39m# feats = self.transformer(feats, feats, tgt_mask=target_mask).reshape((N, -1))\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[39m# print(f'feats shape = {feats.shape}')\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc_out(feats)\n",
      "File \u001b[0;32m/coc/pcba1/bdas31/Miniconda3/envs/haar/lib/python3.8/site-packages/torch/nn/modules/module.py:1502\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1500\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1501\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1502\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/coc/pcba1/bdas31/Miniconda3/envs/haar/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1506\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1507\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1508\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1509\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1510\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1512\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/coc/scratch/bdas31/EPIC-Kitchens_100/HAAR/src/scratch/../models/attention.py:200\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, src, trg)\u001b[0m\n\u001b[1;32m    198\u001b[0m src_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_src_mask(src)\n\u001b[1;32m    199\u001b[0m trg_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_trg_mask(trg)\n\u001b[0;32m--> 200\u001b[0m enc_src \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(src, src_mask)\n\u001b[1;32m    201\u001b[0m \u001b[39m# print(src.shape, enc_src.shape)\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder(trg, enc_src, src_mask, trg_mask)\n",
      "File \u001b[0;32m/coc/pcba1/bdas31/Miniconda3/envs/haar/lib/python3.8/site-packages/torch/nn/modules/module.py:1502\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1500\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1501\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1502\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/coc/pcba1/bdas31/Miniconda3/envs/haar/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1506\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1507\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1508\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1509\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1510\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1512\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/coc/scratch/bdas31/EPIC-Kitchens_100/HAAR/src/scratch/../models/attention.py:96\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, mask):\n\u001b[0;32m---> 96\u001b[0m     batch_size, sequence_len \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mshape\n\u001b[1;32m     97\u001b[0m     positions \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39marange(\u001b[39m0\u001b[39m, sequence_len)\u001b[39m.\u001b[39mexpand(batch_size, sequence_len)\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m     98\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mword_embedding(x) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mposition_embedding(positions))\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "\n",
    "haarModel.to(device)\n",
    "haarModel.train()\n",
    "warnings.filterwarnings('ignore')\n",
    "step_loss = step_acc = 0.\n",
    "for i, batch in enumerate(training_dataset):\n",
    "    labels = batch[0][1]['verb_class']\n",
    "    preds = haarModel(batch).cpu()\n",
    "    loss = loss_fn(preds, labels)\n",
    "    with torch.no_grad():\n",
    "        y = torch.argmax(nn.Softmax(dim=1)(preds), dim=-1)\n",
    "        accuracy = accuracy_score(labels, y)\n",
    "        step_loss += loss.item()\n",
    "        step_acc += accuracy\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if i == 0:\n",
    "        print(f'First step - Loss: {step_loss:.4f} Accuracy: {step_acc:.4f}')\n",
    "\n",
    "    if (i + 1) % 10 == 0:\n",
    "        avg_loss = step_loss / 10\n",
    "        avg_acc = step_acc / 10\n",
    "        print(f'Step {i + 1} - Loss: {avg_loss:.4f} Accuracy: {avg_acc:.4f}')\n",
    "        step_loss = step_acc = 0.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformers_testing_../../data/pilot-02/logs/train_temp_transformer.log\n",
      "transformers_testing_../../data/pilot-02/logs/train_temp_transformer.log\n"
     ]
    }
   ],
   "source": [
    "from train import Trainer\n",
    "# print(device)\n",
    "trainer = Trainer(name='transformers_testing', cfg=cfg, device='cpu')\n",
    "# trainer.train(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]/coc/pcba1/bdas31/Miniconda3/envs/haar/lib/python3.8/site-packages/torch/optim/optimizer.py:243: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  if not is_compiling() and torch.has_cuda and torch.cuda.is_available():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First step - Loss: 4.5295 Accuracy: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/coc/pcba1/bdas31/Miniconda3/envs/haar/lib/python3.8/site-packages/torch/optim/optimizer.py:243: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  if not is_compiling() and torch.has_cuda and torch.cuda.is_available():\n",
      "/coc/pcba1/bdas31/Miniconda3/envs/haar/lib/python3.8/site-packages/torch/optim/optimizer.py:243: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  if not is_compiling() and torch.has_cuda and torch.cuda.is_available():\n",
      "/coc/pcba1/bdas31/Miniconda3/envs/haar/lib/python3.8/site-packages/torch/optim/optimizer.py:243: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  if not is_compiling() and torch.has_cuda and torch.cuda.is_available():\n",
      "/coc/pcba1/bdas31/Miniconda3/envs/haar/lib/python3.8/site-packages/torch/optim/optimizer.py:243: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  if not is_compiling() and torch.has_cuda and torch.cuda.is_available():\n",
      "/coc/pcba1/bdas31/Miniconda3/envs/haar/lib/python3.8/site-packages/torch/optim/optimizer.py:243: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  if not is_compiling() and torch.has_cuda and torch.cuda.is_available():\n",
      "/coc/pcba1/bdas31/Miniconda3/envs/haar/lib/python3.8/site-packages/torch/optim/optimizer.py:243: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  if not is_compiling() and torch.has_cuda and torch.cuda.is_available():\n",
      "/coc/pcba1/bdas31/Miniconda3/envs/haar/lib/python3.8/site-packages/torch/optim/optimizer.py:243: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  if not is_compiling() and torch.has_cuda and torch.cuda.is_available():\n",
      "/coc/pcba1/bdas31/Miniconda3/envs/haar/lib/python3.8/site-packages/torch/optim/optimizer.py:243: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  if not is_compiling() and torch.has_cuda and torch.cuda.is_available():\n",
      "/coc/pcba1/bdas31/Miniconda3/envs/haar/lib/python3.8/site-packages/torch/optim/optimizer.py:243: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  if not is_compiling() and torch.has_cuda and torch.cuda.is_available():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 10 - Loss: 3.8743 Accuracy: 0.0500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/coc/pcba1/bdas31/Miniconda3/envs/haar/lib/python3.8/site-packages/torch/optim/optimizer.py:243: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  if not is_compiling() and torch.has_cuda and torch.cuda.is_available():\n",
      "/coc/pcba1/bdas31/Miniconda3/envs/haar/lib/python3.8/site-packages/torch/optim/optimizer.py:243: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  if not is_compiling() and torch.has_cuda and torch.cuda.is_available():\n",
      "/coc/pcba1/bdas31/Miniconda3/envs/haar/lib/python3.8/site-packages/torch/optim/optimizer.py:243: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  if not is_compiling() and torch.has_cuda and torch.cuda.is_available():\n",
      "/coc/pcba1/bdas31/Miniconda3/envs/haar/lib/python3.8/site-packages/torch/optim/optimizer.py:243: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  if not is_compiling() and torch.has_cuda and torch.cuda.is_available():\n",
      "/coc/pcba1/bdas31/Miniconda3/envs/haar/lib/python3.8/site-packages/torch/optim/optimizer.py:243: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  if not is_compiling() and torch.has_cuda and torch.cuda.is_available():\n",
      "/coc/pcba1/bdas31/Miniconda3/envs/haar/lib/python3.8/site-packages/torch/optim/optimizer.py:243: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  if not is_compiling() and torch.has_cuda and torch.cuda.is_available():\n",
      "/coc/pcba1/bdas31/Miniconda3/envs/haar/lib/python3.8/site-packages/torch/optim/optimizer.py:243: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  if not is_compiling() and torch.has_cuda and torch.cuda.is_available():\n",
      "/coc/pcba1/bdas31/Miniconda3/envs/haar/lib/python3.8/site-packages/torch/optim/optimizer.py:243: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  if not is_compiling() and torch.has_cuda and torch.cuda.is_available():\n",
      "/coc/pcba1/bdas31/Miniconda3/envs/haar/lib/python3.8/site-packages/torch/optim/optimizer.py:243: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  if not is_compiling() and torch.has_cuda and torch.cuda.is_available():\n",
      "/coc/pcba1/bdas31/Miniconda3/envs/haar/lib/python3.8/site-packages/torch/optim/optimizer.py:243: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  if not is_compiling() and torch.has_cuda and torch.cuda.is_available():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 20 - Loss: 4.1270 Accuracy: 0.1250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/coc/pcba1/bdas31/Miniconda3/envs/haar/lib/python3.8/site-packages/torch/optim/optimizer.py:243: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  if not is_compiling() and torch.has_cuda and torch.cuda.is_available():\n",
      "/coc/pcba1/bdas31/Miniconda3/envs/haar/lib/python3.8/site-packages/torch/optim/optimizer.py:243: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  if not is_compiling() and torch.has_cuda and torch.cuda.is_available():\n",
      "/coc/pcba1/bdas31/Miniconda3/envs/haar/lib/python3.8/site-packages/torch/optim/optimizer.py:243: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  if not is_compiling() and torch.has_cuda and torch.cuda.is_available():\n",
      "/coc/pcba1/bdas31/Miniconda3/envs/haar/lib/python3.8/site-packages/torch/optim/optimizer.py:243: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  if not is_compiling() and torch.has_cuda and torch.cuda.is_available():\n",
      "/coc/pcba1/bdas31/Miniconda3/envs/haar/lib/python3.8/site-packages/torch/optim/optimizer.py:243: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  if not is_compiling() and torch.has_cuda and torch.cuda.is_available():\n",
      "/coc/pcba1/bdas31/Miniconda3/envs/haar/lib/python3.8/site-packages/torch/optim/optimizer.py:243: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  if not is_compiling() and torch.has_cuda and torch.cuda.is_available():\n",
      "/coc/pcba1/bdas31/Miniconda3/envs/haar/lib/python3.8/site-packages/torch/optim/optimizer.py:243: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  if not is_compiling() and torch.has_cuda and torch.cuda.is_available():\n",
      "/coc/pcba1/bdas31/Miniconda3/envs/haar/lib/python3.8/site-packages/torch/optim/optimizer.py:243: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  if not is_compiling() and torch.has_cuda and torch.cuda.is_available():\n",
      "/coc/pcba1/bdas31/Miniconda3/envs/haar/lib/python3.8/site-packages/torch/optim/optimizer.py:243: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  if not is_compiling() and torch.has_cuda and torch.cuda.is_available():\n",
      "/coc/pcba1/bdas31/Miniconda3/envs/haar/lib/python3.8/site-packages/torch/optim/optimizer.py:243: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  if not is_compiling() and torch.has_cuda and torch.cuda.is_available():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 30 - Loss: 3.3900 Accuracy: 0.0750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/coc/pcba1/bdas31/Miniconda3/envs/haar/lib/python3.8/site-packages/torch/optim/optimizer.py:243: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  if not is_compiling() and torch.has_cuda and torch.cuda.is_available():\n",
      "/coc/pcba1/bdas31/Miniconda3/envs/haar/lib/python3.8/site-packages/torch/optim/optimizer.py:243: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  if not is_compiling() and torch.has_cuda and torch.cuda.is_available():\n",
      "/coc/pcba1/bdas31/Miniconda3/envs/haar/lib/python3.8/site-packages/torch/optim/optimizer.py:243: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  if not is_compiling() and torch.has_cuda and torch.cuda.is_available():\n",
      "/coc/pcba1/bdas31/Miniconda3/envs/haar/lib/python3.8/site-packages/torch/optim/optimizer.py:243: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  if not is_compiling() and torch.has_cuda and torch.cuda.is_available():\n",
      "/coc/pcba1/bdas31/Miniconda3/envs/haar/lib/python3.8/site-packages/torch/optim/optimizer.py:243: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  if not is_compiling() and torch.has_cuda and torch.cuda.is_available():\n",
      "/coc/pcba1/bdas31/Miniconda3/envs/haar/lib/python3.8/site-packages/torch/optim/optimizer.py:243: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  if not is_compiling() and torch.has_cuda and torch.cuda.is_available():\n",
      "/coc/pcba1/bdas31/Miniconda3/envs/haar/lib/python3.8/site-packages/torch/optim/optimizer.py:243: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  if not is_compiling() and torch.has_cuda and torch.cuda.is_available():\n",
      "/coc/pcba1/bdas31/Miniconda3/envs/haar/lib/python3.8/site-packages/torch/optim/optimizer.py:243: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  if not is_compiling() and torch.has_cuda and torch.cuda.is_available():\n",
      "/coc/pcba1/bdas31/Miniconda3/envs/haar/lib/python3.8/site-packages/torch/optim/optimizer.py:243: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  if not is_compiling() and torch.has_cuda and torch.cuda.is_available():\n",
      "/coc/pcba1/bdas31/Miniconda3/envs/haar/lib/python3.8/site-packages/torch/optim/optimizer.py:243: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  if not is_compiling() and torch.has_cuda and torch.cuda.is_available():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 40 - Loss: 2.8243 Accuracy: 0.2250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/coc/pcba1/bdas31/Miniconda3/envs/haar/lib/python3.8/site-packages/torch/optim/optimizer.py:243: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  if not is_compiling() and torch.has_cuda and torch.cuda.is_available():\n",
      "/coc/pcba1/bdas31/Miniconda3/envs/haar/lib/python3.8/site-packages/torch/optim/optimizer.py:243: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  if not is_compiling() and torch.has_cuda and torch.cuda.is_available():\n",
      "/coc/pcba1/bdas31/Miniconda3/envs/haar/lib/python3.8/site-packages/torch/optim/optimizer.py:243: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  if not is_compiling() and torch.has_cuda and torch.cuda.is_available():\n",
      "/coc/pcba1/bdas31/Miniconda3/envs/haar/lib/python3.8/site-packages/torch/optim/optimizer.py:243: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  if not is_compiling() and torch.has_cuda and torch.cuda.is_available():\n",
      "/coc/pcba1/bdas31/Miniconda3/envs/haar/lib/python3.8/site-packages/torch/optim/optimizer.py:243: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  if not is_compiling() and torch.has_cuda and torch.cuda.is_available():\n",
      "/coc/pcba1/bdas31/Miniconda3/envs/haar/lib/python3.8/site-packages/torch/optim/optimizer.py:243: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  if not is_compiling() and torch.has_cuda and torch.cuda.is_available():\n",
      "/coc/pcba1/bdas31/Miniconda3/envs/haar/lib/python3.8/site-packages/torch/optim/optimizer.py:243: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  if not is_compiling() and torch.has_cuda and torch.cuda.is_available():\n",
      "/coc/pcba1/bdas31/Miniconda3/envs/haar/lib/python3.8/site-packages/torch/optim/optimizer.py:243: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  if not is_compiling() and torch.has_cuda and torch.cuda.is_available():\n",
      "/coc/pcba1/bdas31/Miniconda3/envs/haar/lib/python3.8/site-packages/torch/optim/optimizer.py:243: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  if not is_compiling() and torch.has_cuda and torch.cuda.is_available():\n",
      "/coc/pcba1/bdas31/Miniconda3/envs/haar/lib/python3.8/site-packages/torch/optim/optimizer.py:243: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  if not is_compiling() and torch.has_cuda and torch.cuda.is_available():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 50 - Loss: 3.7703 Accuracy: 0.3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/coc/pcba1/bdas31/Miniconda3/envs/haar/lib/python3.8/site-packages/torch/optim/optimizer.py:243: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  if not is_compiling() and torch.has_cuda and torch.cuda.is_available():\n",
      "/coc/pcba1/bdas31/Miniconda3/envs/haar/lib/python3.8/site-packages/torch/optim/optimizer.py:243: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  if not is_compiling() and torch.has_cuda and torch.cuda.is_available():\n",
      "/coc/pcba1/bdas31/Miniconda3/envs/haar/lib/python3.8/site-packages/torch/optim/optimizer.py:243: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  if not is_compiling() and torch.has_cuda and torch.cuda.is_available():\n",
      "/coc/pcba1/bdas31/Miniconda3/envs/haar/lib/python3.8/site-packages/torch/optim/optimizer.py:243: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  if not is_compiling() and torch.has_cuda and torch.cuda.is_available():\n",
      "/coc/pcba1/bdas31/Miniconda3/envs/haar/lib/python3.8/site-packages/torch/optim/optimizer.py:243: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  if not is_compiling() and torch.has_cuda and torch.cuda.is_available():\n",
      "/coc/pcba1/bdas31/Miniconda3/envs/haar/lib/python3.8/site-packages/torch/optim/optimizer.py:243: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  if not is_compiling() and torch.has_cuda and torch.cuda.is_available():\n",
      "/coc/pcba1/bdas31/Miniconda3/envs/haar/lib/python3.8/site-packages/torch/optim/optimizer.py:243: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  if not is_compiling() and torch.has_cuda and torch.cuda.is_available():\n",
      "/coc/pcba1/bdas31/Miniconda3/envs/haar/lib/python3.8/site-packages/torch/optim/optimizer.py:243: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  if not is_compiling() and torch.has_cuda and torch.cuda.is_available():\n",
      "/coc/pcba1/bdas31/Miniconda3/envs/haar/lib/python3.8/site-packages/torch/optim/optimizer.py:243: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  if not is_compiling() and torch.has_cuda and torch.cuda.is_available():\n",
      "/coc/pcba1/bdas31/Miniconda3/envs/haar/lib/python3.8/site-packages/torch/optim/optimizer.py:243: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  if not is_compiling() and torch.has_cuda and torch.cuda.is_available():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 60 - Loss: 3.5360 Accuracy: 0.1250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/coc/pcba1/bdas31/Miniconda3/envs/haar/lib/python3.8/site-packages/torch/optim/optimizer.py:243: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  if not is_compiling() and torch.has_cuda and torch.cuda.is_available():\n",
      "61it [24:19, 23.92s/it]\n",
      "  0%|          | 0/2 [24:19<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain(\u001b[39m2\u001b[39;49m)\n",
      "File \u001b[0;32m/coc/scratch/bdas31/EPIC-Kitchens_100/HAAR/src/scratch/../train.py:128\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, num_epochs, validate_strategy)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[39mfor\u001b[39;00m i, batch \u001b[39min\u001b[39;00m tqdm(\u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_loader)):\n\u001b[1;32m    126\u001b[0m     \u001b[39m# print(len(batch), len(batch[0]), len(batch[0][0]), type(batch))\u001b[39;00m\n\u001b[1;32m    127\u001b[0m     labels \u001b[39m=\u001b[39m batch[\u001b[39m0\u001b[39m][\u001b[39m1\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mverb_class\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m--> 128\u001b[0m     logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(batch)\n\u001b[1;32m    129\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss(logits, labels)\n\u001b[1;32m    130\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m/coc/pcba1/bdas31/Miniconda3/envs/haar/lib/python3.8/site-packages/torch/nn/modules/module.py:1502\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1500\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1501\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1502\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/coc/pcba1/bdas31/Miniconda3/envs/haar/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1506\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1507\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1508\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1509\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1510\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1512\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/coc/scratch/bdas31/EPIC-Kitchens_100/HAAR/src/scratch/../models/models.py:136\u001b[0m, in \u001b[0;36mHaarModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    134\u001b[0m (rgb, _), (_, _) \u001b[39m=\u001b[39m x\n\u001b[1;32m    135\u001b[0m N \u001b[39m=\u001b[39m rgb\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[0;32m--> 136\u001b[0m feats \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeature_model(x, permute_dims\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    137\u001b[0m target_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_target_mask(feats)\n\u001b[1;32m    138\u001b[0m feats \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformer(feats, feats, tgt_mask\u001b[39m=\u001b[39mtarget_mask) \u001b[39m#? replace target with verb_map embeddings?\u001b[39;00m\n",
      "File \u001b[0;32m/coc/pcba1/bdas31/Miniconda3/envs/haar/lib/python3.8/site-packages/torch/nn/modules/module.py:1502\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1500\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1501\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1502\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/coc/pcba1/bdas31/Miniconda3/envs/haar/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1506\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1507\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1508\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1509\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1510\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1512\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/coc/scratch/bdas31/EPIC-Kitchens_100/HAAR/src/scratch/../models/models.py:101\u001b[0m, in \u001b[0;36mEmbeddingModel.forward\u001b[0;34m(self, x, permute_dims)\u001b[0m\n\u001b[1;32m     99\u001b[0m narration \u001b[39m=\u001b[39m metadata[\u001b[39m'\u001b[39m\u001b[39mnarration\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m    100\u001b[0m rgb_feats \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrgb_model(rgb\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice))\n\u001b[0;32m--> 101\u001b[0m flow_feats \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mflow_model(flow\u001b[39m.\u001b[39;49mto(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice))\n\u001b[1;32m    102\u001b[0m narration_feats \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnarration_model(narration)\n\u001b[1;32m    104\u001b[0m feats \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mhstack([rgb_feats, flow_feats, narration_feats])\n",
      "File \u001b[0;32m/coc/pcba1/bdas31/Miniconda3/envs/haar/lib/python3.8/site-packages/torch/nn/modules/module.py:1502\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1500\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1501\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1502\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/coc/pcba1/bdas31/Miniconda3/envs/haar/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1506\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1507\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1508\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1509\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1510\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1512\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/coc/scratch/bdas31/EPIC-Kitchens_100/HAAR/src/scratch/../models/tsm.py:325\u001b[0m, in \u001b[0;36mTSM.forward\u001b[0;34m(self, input, no_reshape)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_reshape:\n\u001b[1;32m    322\u001b[0m     sample_len \u001b[39m=\u001b[39m (\n\u001b[1;32m    323\u001b[0m         \u001b[39m3\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodality\u001b[39m.\u001b[39mupper() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mRGB\u001b[39m\u001b[39m\"\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39m2\u001b[39m\n\u001b[1;32m    324\u001b[0m     ) \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msegment_length\n\u001b[0;32m--> 325\u001b[0m     base_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbase_model(\u001b[39minput\u001b[39;49m\u001b[39m.\u001b[39;49mview((\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, sample_len) \u001b[39m+\u001b[39;49m \u001b[39minput\u001b[39;49m\u001b[39m.\u001b[39;49msize()[\u001b[39m-\u001b[39;49m\u001b[39m2\u001b[39;49m:]))\n\u001b[1;32m    326\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    327\u001b[0m     base_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase_model(\u001b[39minput\u001b[39m)\n",
      "File \u001b[0;32m/coc/pcba1/bdas31/Miniconda3/envs/haar/lib/python3.8/site-packages/torch/nn/modules/module.py:1502\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1500\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1501\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1502\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/coc/pcba1/bdas31/Miniconda3/envs/haar/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1506\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1507\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1508\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1509\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1510\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1512\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/coc/pcba1/bdas31/Miniconda3/envs/haar/lib/python3.8/site-packages/torchvision/models/resnet.py:285\u001b[0m, in \u001b[0;36mResNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 285\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward_impl(x)\n",
      "File \u001b[0;32m/coc/pcba1/bdas31/Miniconda3/envs/haar/lib/python3.8/site-packages/torchvision/models/resnet.py:276\u001b[0m, in \u001b[0;36mResNet._forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    274\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer2(x)\n\u001b[1;32m    275\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer3(x)\n\u001b[0;32m--> 276\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayer4(x)\n\u001b[1;32m    278\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mavgpool(x)\n\u001b[1;32m    279\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mflatten(x, \u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m/coc/pcba1/bdas31/Miniconda3/envs/haar/lib/python3.8/site-packages/torch/nn/modules/module.py:1502\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1500\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1501\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1502\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/coc/pcba1/bdas31/Miniconda3/envs/haar/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1506\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1507\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1508\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1509\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1510\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1512\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/coc/pcba1/bdas31/Miniconda3/envs/haar/lib/python3.8/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m/coc/pcba1/bdas31/Miniconda3/envs/haar/lib/python3.8/site-packages/torch/nn/modules/module.py:1502\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1500\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1501\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1502\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/coc/pcba1/bdas31/Miniconda3/envs/haar/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1506\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1507\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1508\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1509\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1510\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1512\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/coc/pcba1/bdas31/Miniconda3/envs/haar/lib/python3.8/site-packages/torchvision/models/resnet.py:150\u001b[0m, in \u001b[0;36mBottleneck.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    147\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn1(out)\n\u001b[1;32m    148\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(out)\n\u001b[0;32m--> 150\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv2(out)\n\u001b[1;32m    151\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn2(out)\n\u001b[1;32m    152\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(out)\n",
      "File \u001b[0;32m/coc/pcba1/bdas31/Miniconda3/envs/haar/lib/python3.8/site-packages/torch/nn/modules/module.py:1502\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1500\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1501\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1502\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/coc/pcba1/bdas31/Miniconda3/envs/haar/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1506\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1507\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1508\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1509\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1510\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1512\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/coc/pcba1/bdas31/Miniconda3/envs/haar/lib/python3.8/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m/coc/pcba1/bdas31/Miniconda3/envs/haar/lib/python3.8/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epochs:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epochs:  20%|██        | 1/5 [00:02<00:08,  2.01s/it]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epochs:  40%|████      | 2/5 [00:04<00:06,  2.01s/it]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epochs:  60%|██████    | 3/5 [00:06<00:04,  2.01s/it]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epochs:  80%|████████  | 4/5 [00:08<00:02,  2.01s/it]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epochs: 100%|██████████| 5/5 [00:10<00:00,  2.01s/it]    \n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from tqdm import tqdm\n",
    "from time import sleep\n",
    "a = [(i, i*2 + 1) for i in range(5)]\n",
    "for i, num in tqdm(enumerate(a), total=len(a), desc='epochs'):\n",
    "    sleep(1)\n",
    "    print('new epoch')\n",
    "    for j in tqdm(range(5), desc='inner loop', position=0, leave=False):\n",
    "        sleep(0.2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 384, 100])\n"
     ]
    }
   ],
   "source": [
    "trnsf = nn.Transformer(d_model=100, nhead=4, batch_first=True)\n",
    "attention = trnsf(feats, feats)\n",
    "print(attention.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<NativeLayerNormBackward0 object at 0x7f66bce50610>\n"
     ]
    }
   ],
   "source": [
    "print(attention.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint as pp\n",
    "\n",
    "# pp = pprint()\n",
    "training_dataset = data_module.train_dataloader()\n",
    "eval_dataset = data_module.val_dataloader()\n",
    "# print(iter(training_dataset).device)\n",
    "\n",
    "def train_ddp_huggingface():\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=cfg.model.save_path,\n",
    "        per_device_train_batch_size= cfg.learning.batch_size,\n",
    "        per_device_eval_batch_size=cfg.learning.val_batch_size,\n",
    "        num_train_epochs=5,\n",
    "        evaluation_strategy='epoch',\n",
    "        remove_unused_columns=False,\n",
    "        log_level=\"debug\",\n",
    "        log_on_each_node=False,\n",
    "        logging_dir=cfg.trainer.tb_runs,\n",
    "        logging_steps=cfg.trainer.log_every_n_steps\n",
    "    )\n",
    "\n",
    "\n",
    "    def collate_fn(examples):\n",
    "        '''examples = [(R, F) * bs]\n",
    "        '''\n",
    "\n",
    "        # print(len(examples), len(examples[0]), examples[0][0][0].shape)\n",
    "        rgb_images = []\n",
    "        flow_images = []\n",
    "        labels = []\n",
    "        narration = []\n",
    "        for eg in examples:\n",
    "            rgb_images.append(eg[0][0])\n",
    "            flow_images.append(eg[1][0])\n",
    "            labels.append(eg[0][1]['verb_class'])\n",
    "            narration.append(eg[0][1]['narration'])\n",
    "            # pp.pprint(dct)\n",
    "            # print(eg[0][1])\n",
    "            # print(len(eg[0][1]))\n",
    "            # print(eg[0][0].shape)\n",
    "            # break\n",
    "        # print(len(rgb_images))\n",
    "        # rgb, flow = examples\n",
    "        # rgb_images, metadata = rgb  \n",
    "        # flow_images = flow[0]\n",
    "        # labels = metadata[\"verb_class\"]\n",
    "        # text = metadata[\"narration\"]\n",
    "        return {\n",
    "            \"rgb_images\": torch.stack(rgb_images),\n",
    "            \"flow_images\": torch.stack(flow_images),\n",
    "            \"narration\": narration,\n",
    "            \"labels\": labels,\n",
    "        }\n",
    "\n",
    "\n",
    "    class MyTrainer(Trainer):\n",
    "        def compute_loss(self, model, inputs, return_outputs=False):\n",
    "            # rgb_images = inputs['rgb_images']\n",
    "            # # print(type(rgb_images), len(rgb_images), type(rgb_images[0]))\n",
    "            # rgb_images = torch.tensor([rgb_images])\n",
    "            print(next(system.rgb_model.parameters()).device, inputs['rgb_images'].device)\n",
    "            rgb_feats = system.rgb_model(inputs['rgb_images'])\n",
    "            flow_feats = system.flow_model(inputs['flow_images'])\n",
    "            narration_feats = system.narration_model(input['narration'])\n",
    "            feats = torch.hstack([rgb_feats, flow_feats, narration_feats])\n",
    "\n",
    "            preds = model(feats, inputs['labels'], system.verb_embeddings)\n",
    "            targets=inputs[\"labels\"]\n",
    "            return self.compute_loss(preds, targets), preds if return_outputs else \\\n",
    "                self.compute_loss(preds, targets)\n",
    "        \n",
    "    trainer = MyTrainer(\n",
    "        model,\n",
    "        training_args,\n",
    "        train_dataset=training_dataset.dataset,\n",
    "        eval_dataset=eval_dataset.dataset,\n",
    "        data_collator=collate_fn,\n",
    "    )\n",
    "\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('index', tensor([ 3131, 28357, 18080,  6579, 22033, 17588])), ('Unnamed: 0', tensor([ 3131, 28358, 18080,  6579, 22033, 17588])), ('video_id', ['P02_113', 'P37_101', 'P22_117', 'P03_112', 'P27_103', 'P22_116']), ('root_dir', ['../2g1n6qdydwa9u22shpxqzp0t8m/P02', '../2g1n6qdydwa9u22shpxqzp0t8m/P37', '../2g1n6qdydwa9u22shpxqzp0t8m/P22', '../2g1n6qdydwa9u22shpxqzp0t8m/P03', '../2g1n6qdydwa9u22shpxqzp0t8m/P27', '../2g1n6qdydwa9u22shpxqzp0t8m/P22']), ('narration_id', ['P02_113_106', 'P37_101_216', 'P22_117_405', 'P03_112_65', 'P27_103_306', 'P22_116_136']), ('narration', ['clear chopping board', 'smash garlic', 'remove lid', 'pick up bowl', 'stir liquid', 'pick up spoon']), ('narration_timestamp', ['00:04:47.905', '00:15:02.908', '00:14:56.851', '00:02:05.851', '00:21:54.101', '00:05:19.861']), ('start_frame', tensor([14296, 45046, 44743,  6193, 65606, 15894])), ('end_frame', tensor([14483, 45408, 44918,  6381, 65756, 16044])), ('verb_class', tensor([12, 49, 12,  0, 10,  0])), ('noun_class', tensor([ 18,  51,   6,   7, 150,   1])), ('num_frames', tensor([187, 362, 175, 188, 150, 150]))])\n",
      "OrderedDict([('index', tensor([ 5807,  8098, 28582, 21529, 12990, 16547])), ('Unnamed: 0', tensor([ 5807,  8098, 28583, 21529, 12990, 16547])), ('video_id', ['P03_101', 'P04_107', 'P37_102', 'P27_101', 'P09_105', 'P22_111']), ('root_dir', ['../2g1n6qdydwa9u22shpxqzp0t8m/P03', '../2g1n6qdydwa9u22shpxqzp0t8m/P04', '../2g1n6qdydwa9u22shpxqzp0t8m/P37', '../2g1n6qdydwa9u22shpxqzp0t8m/P27', '../2g1n6qdydwa9u22shpxqzp0t8m/P09', '../2g1n6qdydwa9u22shpxqzp0t8m/P22']), ('narration_id', ['P03_101_329', 'P04_107_392', 'P37_102_63', 'P27_101_275', 'P09_105_9', 'P22_111_266']), ('narration', ['pick up top', 'move greaseproof paper', 'cut carrots into sticks', 'put down fork', 'take plate', 'remove clip']), ('narration_timestamp', ['00:13:21.350', '00:11:54.854', '00:04:16.657', '00:12:17.857', '00:01:00.680', '00:10:56.351']), ('start_frame', tensor([39968, 35643, 12733, 36793,  2935, 32718])), ('end_frame', tensor([40131, 35768, 14358, 36943,  3088, 32831])), ('verb_class', tensor([ 0, 11,  7,  1,  0, 12])), ('noun_class', tensor([ 42,  49,  41,  14,   2, 129])), ('num_frames', tensor([ 163,  125, 1625,  150,  153,  113]))])\n",
      "OrderedDict([('index', tensor([19884, 13265,  4678, 26361, 26232,     4])), ('Unnamed: 0', tensor([19884, 13265,  4678, 26362, 26233,     4])), ('video_id', ['P25_107', 'P11_104', 'P02_129', 'P30_111', 'P30_111', 'P01_103']), ('root_dir', ['../2g1n6qdydwa9u22shpxqzp0t8m/P25', '../2g1n6qdydwa9u22shpxqzp0t8m/P11', '../2g1n6qdydwa9u22shpxqzp0t8m/P02', '../2g1n6qdydwa9u22shpxqzp0t8m/P30', '../2g1n6qdydwa9u22shpxqzp0t8m/P30', '../2g1n6qdydwa9u22shpxqzp0t8m/P01']), ('narration_id', ['P25_107_375', 'P11_104_141', 'P02_129_128', 'P30_111_421', 'P30_111_292', 'P01_103_4']), ('narration', ['pick up aubergine', 'pick up fork', 'open fridge', 'stir passata mixture', 'stir passata mixture', 'close cupboard']), ('narration_timestamp', ['00:27:01.351', '00:10:20.351', '00:04:49.154', '00:17:38.35', '00:11:42.10', '00:00:11.850']), ('start_frame', tensor([80968, 30918, 14358, 52818, 35006,   493])), ('end_frame', tensor([81143, 31043, 14508, 53193, 36368,   668])), ('verb_class', tensor([ 0,  0,  3, 10, 10,  4])), ('noun_class', tensor([67, 14, 12, 71, 71,  3])), ('num_frames', tensor([ 175,  125,  150,  375, 1362,  175]))])\n",
      "OrderedDict([('index', tensor([ 6170,  1666, 25039,  3988, 18828, 10330])), ('Unnamed: 0', tensor([ 6170,  1666, 25040,  3988, 18828, 10330])), ('video_id', ['P03_109', 'P01_109', 'P30_107', 'P02_123', 'P23_102', 'P04_113']), ('root_dir', ['../2g1n6qdydwa9u22shpxqzp0t8m/P03', '../2g1n6qdydwa9u22shpxqzp0t8m/P01', '../2g1n6qdydwa9u22shpxqzp0t8m/P30', '../2g1n6qdydwa9u22shpxqzp0t8m/P02', '../2g1n6qdydwa9u22shpxqzp0t8m/P23', '../2g1n6qdydwa9u22shpxqzp0t8m/P04']), ('narration_id', ['P03_109_116', 'P01_109_403', 'P30_107_441', 'P02_123_67', 'P23_102_361', 'P04_113_318']), ('narration', ['pick up saucepan', 'scoop salt', 'open cupboard', 'close drawer', 'close plate', 'close dishwasher']), ('narration_timestamp', ['00:06:22.472', '00:23:24.357', '00:22:57.60', '00:03:07.907', '00:36:44.101', '00:09:56.603']), ('start_frame', tensor([ 19024,  70118,  68781,   9296, 110106,  29731])), ('end_frame', tensor([ 19222,  70268,  68931,   9458, 110281,  29931])), ('verb_class', tensor([ 0, 16,  3,  4,  4,  4])), ('noun_class', tensor([ 5, 38,  3,  8,  2, 70])), ('num_frames', tensor([198, 150, 150, 162, 175, 200]))])\n",
      "OrderedDict([('index', tensor([ 9843,   877,  4844,  5031, 27216,  1161])), ('Unnamed: 0', tensor([ 9843,   877,  4844,  5031, 27217,  1161])), ('video_id', ['P04_112', 'P01_105', 'P02_129', 'P02_129', 'P35_108', 'P01_107']), ('root_dir', ['../2g1n6qdydwa9u22shpxqzp0t8m/P04', '../2g1n6qdydwa9u22shpxqzp0t8m/P01', '../2g1n6qdydwa9u22shpxqzp0t8m/P02', '../2g1n6qdydwa9u22shpxqzp0t8m/P02', '../2g1n6qdydwa9u22shpxqzp0t8m/P35', '../2g1n6qdydwa9u22shpxqzp0t8m/P01']), ('narration_id', ['P04_112_88', 'P01_105_779', 'P02_129_294', 'P02_129_482', 'P35_108_18', 'P01_107_26']), ('narration', ['put bao bun in tupperware', 'turn off light', 'rinse knife', 'grate parmesan cheese on top of plate', 'put down phone', 'close cereals box']), ('narration_timestamp', ['00:02:27.853', '00:34:55.359', '00:10:17.904', '00:17:44.157', '00:00:51.350', '00:00:51.851']), ('start_frame', tensor([  7293, 104668,  30796,  53108,   2468,   2493])), ('end_frame', tensor([  7468, 104831,  30983,  54483,   2735,   2681])), ('verb_class', tensor([ 5,  8,  2, 77,  1,  4])), ('noun_class', tensor([ 33, 114,   4,  32, 165,  23])), ('num_frames', tensor([ 175,  163,  187, 1375,  267,  188]))])\n",
      "OrderedDict([('index', tensor([ 5882, 16317, 16366, 27391,  8117, 17677])), ('Unnamed: 0', tensor([ 5882, 16317, 16366, 27392,  8117, 17677])), ('video_id', ['P03_102', 'P22_111', 'P22_111', 'P35_108', 'P04_107', 'P22_117']), ('root_dir', ['../2g1n6qdydwa9u22shpxqzp0t8m/P03', '../2g1n6qdydwa9u22shpxqzp0t8m/P22', '../2g1n6qdydwa9u22shpxqzp0t8m/P22', '../2g1n6qdydwa9u22shpxqzp0t8m/P35', '../2g1n6qdydwa9u22shpxqzp0t8m/P04', '../2g1n6qdydwa9u22shpxqzp0t8m/P22']), ('narration_id', ['P03_102_53', 'P22_111_36', 'P22_111_85', 'P35_108_193', 'P04_107_411', 'P22_117_2']), ('narration', ['put butter onto shelf', 'open drawer', 'pick up spoon', 'lather glass', 'lift rolling pin', 'put down pan']), ('narration_timestamp', ['00:01:49.100', '00:01:27.601', '00:03:22.931', '00:11:18.150', '00:12:28.101', '00:00:05.361']), ('start_frame', tensor([ 5356,  4281, 10047, 33808, 37306,   169])), ('end_frame', tensor([ 5606,  4535, 10231, 34281, 37431,   368])), ('verb_class', tensor([ 1,  3,  0,  2, 33,  1])), ('noun_class', tensor([ 78,   8,   1,  10, 136,   5])), ('num_frames', tensor([250, 254, 184, 473, 125, 199]))])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m training_dataset:\n\u001b[1;32m      2\u001b[0m     \u001b[39m# print(len(batch))\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     \u001b[39mprint\u001b[39m(batch[\u001b[39m0\u001b[39m][\u001b[39m1\u001b[39m])\n",
      "File \u001b[0;32m/coc/pcba1/bdas31/Miniconda3/envs/haar/lib/python3.8/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/coc/pcba1/bdas31/Miniconda3/envs/haar/lib/python3.8/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/coc/pcba1/bdas31/Miniconda3/envs/haar/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/coc/pcba1/bdas31/Miniconda3/envs/haar/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/coc/scratch/bdas31/EPIC-Kitchens_100/HAAR/src/scratch/../datasets/haar_dataset.py:14\u001b[0m, in \u001b[0;36mHaarDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, index):\n\u001b[0;32m---> 14\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrgb_dataset[index], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mflow_dataset[index]\n",
      "File \u001b[0;32m/coc/scratch/bdas31/EPIC-Kitchens_100/HAAR/src/scratch/../datasets/tsn_dataset.py:63\u001b[0m, in \u001b[0;36mTsnDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     58\u001b[0m     segment_start_idxs \u001b[39m=\u001b[39m (\n\u001b[1;32m     59\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sample_indices(record)\n\u001b[1;32m     60\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrandom_shift\n\u001b[1;32m     61\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_val_indices(record)\n\u001b[1;32m     62\u001b[0m     )\n\u001b[0;32m---> 63\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get(record, segment_start_idxs)\n",
      "File \u001b[0;32m/coc/scratch/bdas31/EPIC-Kitchens_100/HAAR/src/scratch/../datasets/tsn_dataset.py:70\u001b[0m, in \u001b[0;36mTsnDataset._get\u001b[0;34m(self, record, segment_start_idxs)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[39m@profile\u001b[39m\n\u001b[1;32m     69\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get\u001b[39m(\u001b[39mself\u001b[39m, record: VideoRecord, segment_start_idxs: List[\u001b[39mint\u001b[39m]):\n\u001b[0;32m---> 70\u001b[0m     images \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset\u001b[39m.\u001b[39;49mload_frames(\n\u001b[1;32m     71\u001b[0m         record, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_frame_idxs(segment_start_idxs, record)\n\u001b[1;32m     72\u001b[0m     )\n\u001b[1;32m     73\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m         images \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform(images)\n",
      "File \u001b[0;32m/coc/scratch/bdas31/EPIC-Kitchens_100/HAAR/src/scratch/../datasets/epic_dataset.py:103\u001b[0m, in \u001b[0;36mEpicVideoDataset.load_frames\u001b[0;34m(self, record, indices)\u001b[0m\n\u001b[1;32m     98\u001b[0m selected_frames: List[FramesTypeVar] \u001b[39m=\u001b[39m []\n\u001b[1;32m     99\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m indices:\n\u001b[1;32m    100\u001b[0m     \u001b[39m# Without passing a slice to the gulp directory index we load ALL the frames\u001b[39;00m\n\u001b[1;32m    101\u001b[0m     \u001b[39m# so we create a slice with a single element -- that way we only read a single frame\u001b[39;00m\n\u001b[1;32m    102\u001b[0m     \u001b[39m# from the gulp chunk, and not the whole chunk.\u001b[39;00m\n\u001b[0;32m--> 103\u001b[0m     frames \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sample_video_at_index(cast(GulpVideoRecord, record), i)\n\u001b[1;32m    104\u001b[0m     frames \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msample_transform(frames)\n\u001b[1;32m    105\u001b[0m     selected_frames\u001b[39m.\u001b[39mextend(frames)\n",
      "File \u001b[0;32m/coc/scratch/bdas31/EPIC-Kitchens_100/HAAR/src/scratch/../datasets/epic_dataset.py:166\u001b[0m, in \u001b[0;36mEpicVideoDataset._sample_video_at_index\u001b[0;34m(self, record, index)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[39m@profile\u001b[39m\n\u001b[1;32m    162\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_sample_video_at_index\u001b[39m(\n\u001b[1;32m    163\u001b[0m     \u001b[39mself\u001b[39m, record: GulpVideoRecord, index: \u001b[39mint\u001b[39m\n\u001b[1;32m    164\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[PIL\u001b[39m.\u001b[39mImage\u001b[39m.\u001b[39mImage]:\n\u001b[1;32m    165\u001b[0m     single_frame_slice \u001b[39m=\u001b[39m \u001b[39mslice\u001b[39m(index, index \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[0;32m--> 166\u001b[0m     numpy_frame \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgulp_dir[record\u001b[39m.\u001b[39;49mgulp_index, single_frame_slice][\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\n\u001b[1;32m    167\u001b[0m     \u001b[39mreturn\u001b[39;00m [PIL\u001b[39m.\u001b[39mImage\u001b[39m.\u001b[39mfromarray(numpy_frame)\u001b[39m.\u001b[39mconvert(\u001b[39m\"\u001b[39m\u001b[39mRGB\u001b[39m\u001b[39m\"\u001b[39m)]\n",
      "File \u001b[0;32m/coc/pcba1/bdas31/Miniconda3/envs/haar/lib/python3.8/site-packages/gulpio2/fileio.py:142\u001b[0m, in \u001b[0;36mGulpDirectory.__getitem__\u001b[0;34m(self, element)\u001b[0m\n\u001b[1;32m    140\u001b[0m gulp_chunk \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchunk_objs_lookup[chunk_id]\n\u001b[1;32m    141\u001b[0m \u001b[39mwith\u001b[39;00m gulp_chunk\u001b[39m.\u001b[39mopen():\n\u001b[0;32m--> 142\u001b[0m     \u001b[39mreturn\u001b[39;00m gulp_chunk[element]\n",
      "File \u001b[0;32m/coc/pcba1/bdas31/Miniconda3/envs/haar/lib/python3.8/site-packages/gulpio2/fileio.py:223\u001b[0m, in \u001b[0;36mGulpChunk.__getitem__\u001b[0;34m(self, element)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, element):\n\u001b[1;32m    222\u001b[0m     id_, slice_ \u001b[39m=\u001b[39m extract_input_for_getitem(element)\n\u001b[0;32m--> 223\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread_frames(id_, slice_)\n",
      "File \u001b[0;32m/coc/pcba1/bdas31/Miniconda3/envs/haar/lib/python3.8/site-packages/gulpio2/fileio.py:360\u001b[0m, in \u001b[0;36mGulpChunk.read_frames\u001b[0;34m(self, id_, slice_)\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    359\u001b[0m     selected_frame_infos \u001b[39m=\u001b[39m frame_infos[slice_element]\n\u001b[0;32m--> 360\u001b[0m frames \u001b[39m=\u001b[39m [extract_frame(frame_info)\n\u001b[1;32m    361\u001b[0m           \u001b[39mfor\u001b[39;00m frame_info \u001b[39min\u001b[39;00m selected_frame_infos]\n\u001b[1;32m    362\u001b[0m \u001b[39mreturn\u001b[39;00m frames, meta_data\n",
      "File \u001b[0;32m/coc/pcba1/bdas31/Miniconda3/envs/haar/lib/python3.8/site-packages/gulpio2/fileio.py:360\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    359\u001b[0m     selected_frame_infos \u001b[39m=\u001b[39m frame_infos[slice_element]\n\u001b[0;32m--> 360\u001b[0m frames \u001b[39m=\u001b[39m [extract_frame(frame_info)\n\u001b[1;32m    361\u001b[0m           \u001b[39mfor\u001b[39;00m frame_info \u001b[39min\u001b[39;00m selected_frame_infos]\n\u001b[1;32m    362\u001b[0m \u001b[39mreturn\u001b[39;00m frames, meta_data\n",
      "File \u001b[0;32m/coc/pcba1/bdas31/Miniconda3/envs/haar/lib/python3.8/site-packages/gulpio2/fileio.py:352\u001b[0m, in \u001b[0;36mGulpChunk.read_frames.<locals>.extract_frame\u001b[0;34m(frame_info)\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mextract_frame\u001b[39m(frame_info):\n\u001b[1;32m    351\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp\u001b[39m.\u001b[39mseek(frame_info\u001b[39m.\u001b[39mloc)\n\u001b[0;32m--> 352\u001b[0m     record \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mread(frame_info\u001b[39m.\u001b[39;49mlength)\n\u001b[1;32m    353\u001b[0m     img_str \u001b[39m=\u001b[39m record[:\u001b[39mlen\u001b[39m(record)\u001b[39m-\u001b[39mframe_info\u001b[39m.\u001b[39mpad]\n\u001b[1;32m    354\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjpeg_decoder(img_str)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for batch in training_dataset:\n",
    "    # print(len(batch))\n",
    "    print(batch[0][1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import notebook_launcher\n",
    "# os.environ['TOKENIZERS_PARALLELISM']=\"false\"\n",
    "notebook_launcher(train_ddp_huggingface, args=(), num_processes=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
