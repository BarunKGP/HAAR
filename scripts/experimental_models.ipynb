{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/coc/scratch/bdas31/EPIC-Kitchens_100/model.ipynb Cell 1\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bzatopek.cc.gatech.edu/coc/scratch/bdas31/EPIC-Kitchens_100/model.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bzatopek.cc.gatech.edu/coc/scratch/bdas31/EPIC-Kitchens_100/model.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mPIL\u001b[39;00m \u001b[39mimport\u001b[39;00m Image, ImageDraw\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bzatopek.cc.gatech.edu/coc/scratch/bdas31/EPIC-Kitchens_100/model.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import pickle\n",
    "from typing import Any, Callable, List, Optional, Tuple\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "# import PIL\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Helper functions (taken from CV Project 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_img(img: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Bring image values to [0,1] range\n",
    "    Args:\n",
    "        img: (H,W,C) or (H,W) image\n",
    "    \"\"\"\n",
    "    img -= img.min()\n",
    "    img /= img.max()\n",
    "    return img\n",
    "\n",
    "\n",
    "# def verify(function: Callable) -> str:\n",
    "#     \"\"\"Will indicate with a print statement whether assertions passed or failed\n",
    "#     within function argument call.\n",
    "#     Args:\n",
    "#         function: Python function object\n",
    "#     Returns:\n",
    "#         string that is colored red or green when printed, indicating success\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         function()\n",
    "#         return '\\x1b[32m\"Correct\"\\x1b[0m'\n",
    "#     except AssertionError:\n",
    "#         return '\\x1b[31m\"Wrong\"\\x1b[0m'\n",
    "\n",
    "\n",
    "def rgb2gray(img: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Use the coefficients used in OpenCV, found here:\n",
    "    https://docs.opencv.org/3.4/de/d25/imgproc_color_conversions.html\n",
    "    Args:\n",
    "        Numpy array of shape (M,N,3) representing RGB image in HWC format\n",
    "    Returns:\n",
    "        Numpy array of shape (M,N) representing grayscale image\n",
    "    \"\"\"\n",
    "    # Grayscale coefficients\n",
    "    c = [0.299, 0.587, 0.114]\n",
    "    return img[:, :, 0] * c[0] + img[:, :, 1] * c[1] + img[:, :, 2] * c[2]\n",
    "\n",
    "\n",
    "def PIL_resize(img: np.ndarray, size: Tuple[int, int]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        img: Array representing an image\n",
    "        size: Tuple representing new desired (width, height)\n",
    "    Returns:\n",
    "        img\n",
    "    \"\"\"\n",
    "    img = numpy_arr_to_PIL_image(img, scale_to_255=True)\n",
    "    img = img.resize(size)\n",
    "    img = PIL_image_to_numpy_arr(img)\n",
    "    return img\n",
    "\n",
    "\n",
    "def PIL_image_to_numpy_arr(img: Image, downscale_by_255: bool = True) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        img: PIL Image\n",
    "        downscale_by_255: whether to divide uint8 values by 255 to normalize\n",
    "        values to range [0,1]\n",
    "    Returns:\n",
    "        img\n",
    "    \"\"\"\n",
    "    img = np.asarray(img)\n",
    "    img = img.astype(np.float32)\n",
    "    if downscale_by_255:\n",
    "        img /= 255\n",
    "    return img\n",
    "\n",
    "\n",
    "def im2single(im: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        img: uint8 array of shape (m,n,c) or (m,n) and in range [0,255]\n",
    "    Returns:\n",
    "        im: float or double array of identical shape and in range [0,1]\n",
    "    \"\"\"\n",
    "    im = im.astype(np.float32) / 255\n",
    "    return im\n",
    "\n",
    "\n",
    "def single2im(im: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        im: float or double array of shape (m,n,c) or (m,n) and in range [0,1]\n",
    "    Returns:\n",
    "        im: uint8 array of identical shape and in range [0,255]\n",
    "    \"\"\"\n",
    "    im *= 255\n",
    "    im = im.astype(np.uint8)\n",
    "    return im\n",
    "\n",
    "\n",
    "def numpy_arr_to_PIL_image(img: np.ndarray, scale_to_255: False) -> PIL.Image:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        img: in [0,1]\n",
    "    Returns:\n",
    "        img in [0,255]\n",
    "    \"\"\"\n",
    "    if scale_to_255:\n",
    "        img *= 255\n",
    "    return PIL.Image.fromarray(np.uint8(img))\n",
    "\n",
    "\n",
    "def load_image(path: str) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        path: string representing a file path to an image\n",
    "    Returns:\n",
    "        float_img_rgb: float or double array of shape (m,n,c) or (m,n)\n",
    "           and in range [0,1], representing an RGB image\n",
    "    \"\"\"\n",
    "    img = PIL.Image.open(path)\n",
    "    img = np.asarray(img, dtype=float)\n",
    "    float_img_rgb = im2single(img)\n",
    "    return float_img_rgb\n",
    "\n",
    "\n",
    "def save_image(path: str, im: np.ndarray) -> None:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        path: string representing a file path to an image\n",
    "        img: numpy array\n",
    "    \"\"\"\n",
    "    folder_path = os.path.split(path)[0]\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "    img = copy.deepcopy(im)\n",
    "    img = single2im(img)\n",
    "    pil_img = numpy_arr_to_PIL_image(img, scale_to_255=False)\n",
    "    pil_img.save(path)\n",
    "\n",
    "\n",
    "def hstack_images(img1, img2):\n",
    "    \"\"\"\n",
    "    Stacks 2 images side-by-side and creates one combined image.\n",
    "    Args:\n",
    "    - imgA: A numpy array of shape (M,N,3) representing rgb image\n",
    "    - imgB: A numpy array of shape (D,E,3) representing rgb image\n",
    "    Returns:\n",
    "    - newImg: A numpy array of shape (max(M,D), N+E, 3)\n",
    "    \"\"\"\n",
    "\n",
    "    # CHANGED\n",
    "    imgA = np.array(img1)\n",
    "    imgB = np.array(img2)\n",
    "    Height = max(imgA.shape[0], imgB.shape[0])\n",
    "    Width = imgA.shape[1] + imgB.shape[1]\n",
    "\n",
    "    newImg = np.zeros((Height, Width, 3), dtype=imgA.dtype)\n",
    "    newImg[: imgA.shape[0], : imgA.shape[1], :] = imgA\n",
    "    newImg[: imgB.shape[0], imgA.shape[1] :, :] = imgB\n",
    "\n",
    "    # newImg = PIL.Image.fromarray(np.uint8(newImg))\n",
    "    return newImg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing frames\n",
    "import PIL\n",
    "import numpy as np\n",
    "p_img = PIL.Image.open()\n",
    "def PIL_image_to_numpy_arr(img: Image, downscale_by_255: bool = True) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        img: PIL Image\n",
    "        downscale_by_255: whether to divide uint8 values by 255 to normalize\n",
    "        values to range [0,1]\n",
    "    Returns:\n",
    "        img\n",
    "    \"\"\"\n",
    "    img = np.asarray(img)\n",
    "    img = img.astype(np.float32)\n",
    "    if downscale_by_255:\n",
    "        img /= 255\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P02_105.MP4: 1080.0 x 1920.0\n",
      "P02_102.MP4: 1080.0 x 1920.0\n",
      "P02_137.MP4: 1080.0 x 1920.0\n",
      "P02_130.MP4: 1080.0 x 1920.0\n",
      "P02_121.MP4: 1080.0 x 1920.0\n",
      "P02_126.MP4: 1080.0 x 1920.0\n",
      "P02_128.MP4: 1080.0 x 1920.0\n",
      "P02_113.MP4: 1080.0 x 1920.0\n",
      "P02_114.MP4: 1080.0 x 1920.0\n",
      "P02_131.MP4: 1080.0 x 1920.0\n",
      "P02_136.MP4: 1080.0 x 1920.0\n",
      "P02_103.MP4: 1080.0 x 1920.0\n",
      "P02_104.MP4: 1080.0 x 1920.0\n",
      "P02_115.MP4: 1080.0 x 1920.0\n",
      "P02_112.MP4: 1080.0 x 1920.0\n",
      "P02_129.MP4: 1080.0 x 1920.0\n",
      "P02_127.MP4: 1080.0 x 1920.0\n",
      "P02_120.MP4: 1080.0 x 1920.0\n",
      "P02_111.MP4: 1080.0 x 1920.0\n",
      "P02_116.MP4: 1080.0 x 1920.0\n",
      "P02_118.MP4: 1080.0 x 1920.0\n",
      "P02_123.MP4: 1080.0 x 1920.0\n",
      "P02_124.MP4: 1080.0 x 1920.0\n",
      "P02_135.MP4: 1080.0 x 1920.0\n",
      "P02_132.MP4: 1080.0 x 1920.0\n",
      "P02_109.MP4: 1080.0 x 1920.0\n",
      "P02_107.MP4: 1080.0 x 1920.0\n",
      "P02_125.MP4: 1080.0 x 1920.0\n",
      "P02_122.MP4: 1080.0 x 1920.0\n",
      "P02_119.MP4: 1080.0 x 1920.0\n",
      "P02_117.MP4: 1080.0 x 1920.0\n",
      "P02_110.MP4: 1080.0 x 1920.0\n",
      "P02_101.MP4: 1080.0 x 1920.0\n",
      "P02_106.MP4: 1080.0 x 1920.0\n",
      "P02_108.MP4: 1080.0 x 1920.0\n",
      "P02_133.MP4: 1080.0 x 1920.0\n",
      "P02_134.MP4: 1080.0 x 1920.0\n"
     ]
    }
   ],
   "source": [
    "file_path = \"2g1n6qdydwa9u22shpxqzp0t8m/P02/videos/\"  # change to your own video path\n",
    "files = os.listdir(file_path)\n",
    "for f in files:\n",
    "    path = file_path + f\n",
    "    vid = cv2.VideoCapture(path)\n",
    "    height = vid.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "    width = vid.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
    "    print(f'{f}: {height} x {width}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2', device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding shape = (384,)\n",
      "embedding = [-2.05574054e-02  5.83467074e-02 -3.61822508e-02 -3.85913299e-03\n",
      " -1.02967463e-01 -2.12387722e-02  1.13360491e-02  2.05003992e-02\n",
      " -1.21942088e-02 -5.22209052e-03  1.22582912e-01 -5.20467274e-02\n",
      " -8.58943015e-02  4.99771833e-02 -5.25189042e-02 -1.00123577e-01\n",
      "  5.90349883e-02  1.52454283e-02  4.67250356e-03  3.19586731e-02\n",
      " -6.30533546e-02  9.30613186e-03  2.04243120e-02  5.70746465e-03\n",
      " -2.00125668e-02  8.88943523e-02 -1.95299729e-03  6.55974448e-02\n",
      " -2.26503108e-02 -1.94703061e-02 -1.39277875e-02 -5.86142875e-02\n",
      " -1.85706001e-02  9.50347260e-03 -1.17636267e-02  5.33422790e-02\n",
      " -3.14691029e-02  7.37755150e-02 -3.37130018e-02 -2.92711556e-02\n",
      " -3.96612287e-02  1.60753354e-02  4.74423617e-02 -2.67440956e-02\n",
      "  8.83961320e-02  4.18313034e-02 -1.40460934e-02  2.41288748e-02\n",
      "  9.60046947e-02 -3.51824500e-02 -2.10426897e-02  8.50035995e-03\n",
      " -2.69936845e-02 -8.40251222e-02  3.78841348e-03  5.67627586e-02\n",
      "  2.86039966e-03  6.23897687e-02  4.86245239e-03  3.77090201e-02\n",
      "  5.61857000e-02 -2.51110131e-03 -5.27414493e-02  1.09111154e-02\n",
      " -5.68424771e-03 -2.94399466e-02  3.14119607e-02 -3.51963229e-02\n",
      " -1.21385688e-02  5.83947301e-02  1.04132392e-01 -8.59086961e-03\n",
      " -3.32995616e-02 -3.65892351e-02  4.57366519e-02 -3.37201841e-02\n",
      "  3.69035378e-02 -1.31917568e-02 -7.39423325e-03  5.97439706e-02\n",
      " -8.63396078e-02  5.01946136e-02 -4.37063398e-03  8.25413689e-02\n",
      " -2.16894336e-02 -9.60337406e-04 -4.46637198e-02  2.88252365e-02\n",
      " -4.11921553e-02  2.11008675e-02  5.64929731e-02  6.33121375e-03\n",
      "  2.22903993e-02  4.88889664e-02 -2.71790028e-02 -3.71125690e-03\n",
      " -4.69169877e-02  2.90289856e-02 -6.73516244e-02  8.03011060e-02\n",
      "  1.50129292e-03 -3.67771499e-02 -5.47415428e-02  5.74848130e-02\n",
      "  1.48336506e-02  3.98187749e-02  3.74417678e-02  1.10421348e-02\n",
      " -7.40919914e-03 -6.75111935e-02  1.23759015e-02  1.16102934e-01\n",
      " -8.05766582e-02  1.84168555e-02  3.78201460e-03 -5.77177964e-02\n",
      "  1.43785188e-02 -7.68762454e-03 -1.28434310e-02 -1.51711665e-02\n",
      " -2.32949946e-02  4.69124392e-02 -3.94090861e-02  2.14739181e-02\n",
      " -4.06720452e-02 -4.30866629e-02  7.63741285e-02 -7.59946583e-34\n",
      "  1.59689356e-02  4.74745519e-02 -1.28739988e-02 -3.65998149e-02\n",
      "  2.52671205e-02 -1.48745868e-02  3.20562124e-02  3.94064561e-02\n",
      " -3.68336798e-03  5.74109331e-02  6.41624704e-02 -1.25240207e-01\n",
      " -4.77122143e-02 -1.28972605e-02  1.13063743e-02 -6.54694587e-02\n",
      "  2.72655413e-02  5.95409088e-02 -1.02511691e-02 -1.82835422e-02\n",
      "  3.96310464e-02 -5.24440259e-02  1.64273046e-02  1.30389901e-02\n",
      "  1.48920482e-02  1.01348750e-01  4.47389409e-02 -1.08435482e-01\n",
      " -2.99148518e-03  3.25496644e-02 -2.37346143e-02  2.64745671e-02\n",
      "  6.21562302e-02 -5.99558987e-02 -8.33428502e-02  7.72501677e-02\n",
      " -3.90234850e-02 -1.45618143e-02 -5.12269437e-02 -5.64412475e-02\n",
      "  4.20339331e-02  7.35333860e-02 -1.32353544e-01  1.29473517e-02\n",
      "  5.78358844e-02 -6.57243431e-02  5.93156256e-02  7.56938979e-02\n",
      " -7.73220742e-03 -6.02284633e-03  2.81171054e-02 -2.98926346e-02\n",
      "  1.39839947e-01  1.70649942e-02 -1.21502936e-01  2.09643506e-02\n",
      "  2.79560518e-02  5.42972833e-02 -1.46689145e-02 -1.22198649e-03\n",
      "  7.30316043e-02  5.45420200e-02 -1.97599716e-02  1.08714215e-01\n",
      " -3.33114527e-02 -1.01992078e-02  2.12936271e-02 -3.32102887e-02\n",
      " -7.24731851e-03 -6.39490411e-02 -5.05548306e-02 -8.20129365e-02\n",
      " -7.47132255e-03  4.07407358e-02 -1.07815593e-01  3.46369334e-02\n",
      " -2.51374412e-02  1.01299491e-03 -6.43282756e-02 -7.10093379e-02\n",
      " -3.62501964e-02  1.60611067e-02 -3.18470388e-03 -3.11840829e-02\n",
      "  2.92653386e-02  4.06251773e-02  6.61704987e-02 -3.91742177e-02\n",
      "  1.04963146e-01  1.44027937e-02 -9.15029496e-02 -1.01110242e-01\n",
      "  2.40404885e-02 -4.86850105e-02  3.17338109e-02  8.58601553e-34\n",
      "  8.32999125e-03  2.02219859e-02  1.58541221e-02 -1.88859627e-02\n",
      " -1.61230396e-02 -3.13204080e-02 -9.42986365e-03  2.80065946e-02\n",
      " -6.90304339e-02  2.18697195e-03 -9.54651162e-02 -1.99823752e-02\n",
      " -2.39816997e-02 -4.74766567e-02  3.57606187e-02  3.05436291e-02\n",
      "  6.30233809e-03  4.52462062e-02 -7.09448904e-02  2.65845563e-02\n",
      " -4.71016690e-02  1.26955425e-02  1.02392510e-01  4.32671010e-02\n",
      " -7.05393124e-03  2.14866232e-02  3.71673033e-02  3.92108634e-02\n",
      " -4.97700535e-02 -4.09111008e-02 -7.06475154e-02 -7.95041546e-02\n",
      "  3.33841257e-02  3.41344848e-02 -8.39342251e-02  1.23542778e-01\n",
      " -6.40724897e-02  3.60648707e-02 -1.77885573e-02  1.00929372e-03\n",
      " -3.48484106e-02  6.22074008e-02 -3.34592685e-02  1.13469198e-01\n",
      " -9.16355103e-02 -7.03464670e-04  6.87980950e-02 -2.18586139e-02\n",
      " -3.17254057e-03  1.07489496e-01  2.27670022e-03  4.75357147e-03\n",
      "  2.66595315e-02 -1.44764744e-02  1.86573286e-02 -2.32644589e-03\n",
      "  5.31682186e-02 -9.20325965e-02 -5.43454885e-02 -5.63440397e-02\n",
      " -1.86068402e-03  3.08902245e-02  1.17519209e-02  1.07024074e-01\n",
      "  1.48863941e-01 -8.46045092e-02 -1.27119636e-02 -4.23047459e-03\n",
      " -7.21643716e-02  1.56678259e-02 -9.95822474e-02  9.22306553e-02\n",
      "  9.58359912e-02  1.35908397e-02  3.14169633e-03 -2.86140312e-02\n",
      " -3.17554772e-02 -4.43307534e-02 -2.22526826e-02  2.86840368e-03\n",
      " -1.29928561e-02 -1.12027153e-01 -9.03281756e-03  5.05539253e-02\n",
      "  8.28783512e-02 -5.08299805e-02  6.20822329e-03  4.36508320e-02\n",
      " -7.19830096e-02 -4.80250310e-04 -4.78957705e-02  1.07069816e-02\n",
      "  1.44623056e-01  2.75018308e-02  4.22758125e-02 -1.44627670e-08\n",
      "  5.96296154e-02  3.84774408e-03 -1.99454594e-02  1.43623473e-02\n",
      " -3.21531743e-02  4.52717021e-03 -6.51194453e-02 -4.75096367e-02\n",
      " -1.28694493e-02 -1.00945331e-01  3.47459759e-03 -1.31913079e-02\n",
      "  1.33924400e-02  3.22423801e-02 -1.19653612e-01  9.85966157e-03\n",
      "  1.26238940e-02 -1.59617998e-02 -1.79729685e-02 -6.58238083e-02\n",
      " -3.13527212e-02 -2.22850908e-02  1.04387164e-01 -4.09734761e-03\n",
      "  1.98770929e-02  1.42562212e-02 -3.52826901e-02  5.52019253e-02\n",
      "  2.10513324e-02  5.98157011e-02  2.21792050e-02 -5.03811911e-02\n",
      "  1.12855837e-01  4.46471162e-02  9.40804034e-02  1.93207758e-03\n",
      " -6.25532717e-02  1.90196615e-02  9.16200224e-03 -6.85694255e-03\n",
      " -7.56067038e-03 -1.46589326e-02 -6.48247972e-02  5.13796546e-02\n",
      " -7.78417140e-02 -1.22597665e-02  3.28185037e-02  2.69824918e-02\n",
      "  4.75052919e-04 -1.34382062e-02  4.79360037e-02 -4.82158922e-02\n",
      " -3.79638816e-03 -2.92603020e-02  2.89748311e-02  9.76091027e-02\n",
      "  5.52766435e-02 -4.83577177e-02  3.51479501e-02  3.19772363e-02\n",
      " -7.24112466e-02 -7.51471659e-03 -6.74696863e-02 -4.13919464e-02]\n"
     ]
    }
   ],
   "source": [
    "sentence = 'take knife and put back plate'\n",
    "embedding = model.encode(sentence)\n",
    "print(f'embedding shape = {embedding.shape}')\n",
    "print(f'embedding = {embedding}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Boilerplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, dim_model, dropout_p, max_len):\n",
    "        super().__init__()\n",
    "        # Modified version from: https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "        # max_len determines how far the position can have an effect on a token (window)\n",
    "        \n",
    "        # Info\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        \n",
    "        # Encoding - From formula\n",
    "        pos_encoding = torch.zeros(max_len, dim_model)\n",
    "        positions_list = torch.arange(0, max_len, dtype=torch.float).view(-1, 1) # 0, 1, 2, 3, 4, 5\n",
    "        division_term = torch.exp(torch.arange(0, dim_model, 2).float() * (-math.log(10000.0)) / dim_model) # 1000^(2i/dim_model)\n",
    "        \n",
    "        # PE(pos, 2i) = sin(pos/1000^(2i/dim_model))\n",
    "        pos_encoding[:, 0::2] = torch.sin(positions_list * division_term)\n",
    "        \n",
    "        # PE(pos, 2i + 1) = cos(pos/1000^(2i/dim_model))\n",
    "        pos_encoding[:, 1::2] = torch.cos(positions_list * division_term)\n",
    "        \n",
    "        # Saving buffer (same as parameter without gradients needed)\n",
    "        pos_encoding = pos_encoding.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer(\"pos_encoding\",pos_encoding)\n",
    "        \n",
    "    def forward(self, token_embedding: torch.tensor) -> torch.tensor:\n",
    "        # Residual connection + pos encoding\n",
    "        return self.dropout(token_embedding + self.pos_encoding[:token_embedding.size(0), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Model from \"A detailed guide to Pytorch's nn.Transformer() module.\", by\n",
    "    Daniel Melchor: https://medium.com/p/c80afbc9ffb1/\n",
    "    \"\"\"\n",
    "    # Constructor\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_tokens,\n",
    "        dim_model,\n",
    "        num_heads,\n",
    "        num_encoder_layers,\n",
    "        num_decoder_layers,\n",
    "        dropout_p,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # INFO\n",
    "        self.model_type = \"Transformer\"\n",
    "        self.dim_model = dim_model\n",
    "\n",
    "        # LAYERS\n",
    "        self.positional_encoder = PositionalEncoding(\n",
    "            dim_model=dim_model, dropout_p=dropout_p, max_len=5000\n",
    "        )\n",
    "        self.embedding = nn.Embedding(num_tokens, dim_model)\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=dim_model,\n",
    "            nhead=num_heads,\n",
    "            num_encoder_layers=num_encoder_layers,\n",
    "            num_decoder_layers=num_decoder_layers,\n",
    "            dropout=dropout_p,\n",
    "        )\n",
    "        self.out = nn.Linear(dim_model, num_tokens)\n",
    "        \n",
    "    def forward(self, src, tgt, tgt_mask=None, src_pad_mask=None, tgt_pad_mask=None):\n",
    "        # Src size must be (batch_size, src sequence length)\n",
    "        # Tgt size must be (batch_size, tgt sequence length)\n",
    "\n",
    "        # Embedding + positional encoding - Out size = (batch_size, sequence length, dim_model)\n",
    "        src = self.embedding(src) * math.sqrt(self.dim_model)\n",
    "        tgt = self.embedding(tgt) * math.sqrt(self.dim_model)\n",
    "        src = self.positional_encoder(src)\n",
    "        tgt = self.positional_encoder(tgt)\n",
    "        \n",
    "        # We could use the parameter batch_first=True, but our KDL version doesn't support it yet, so we permute\n",
    "        # to obtain size (sequence length, batch_size, dim_model),\n",
    "        src = src.permute(1,0,2)\n",
    "        tgt = tgt.permute(1,0,2)\n",
    "\n",
    "        # Transformer blocks - Out size = (sequence length, batch_size, num_tokens)\n",
    "        transformer_out = self.transformer(src, tgt, tgt_mask=tgt_mask, src_key_padding_mask=src_pad_mask, tgt_key_padding_mask=tgt_pad_mask)\n",
    "        out = self.out(transformer_out)\n",
    "        \n",
    "        return out\n",
    "      \n",
    "    def get_tgt_mask(self, size) -> torch.tensor:\n",
    "        # Generates a squeare matrix where the each row allows one word more to be seen\n",
    "        mask = torch.tril(torch.ones(size, size) == 1) # Lower triangular matrix\n",
    "        mask = mask.float()\n",
    "        mask = mask.masked_fill(mask == 0, float('-inf')) # Convert zeros to -inf\n",
    "        mask = mask.masked_fill(mask == 1, float(0.0)) # Convert ones to 0\n",
    "        \n",
    "        # EX for size=5:\n",
    "        # [[0., -inf, -inf, -inf, -inf],\n",
    "        #  [0.,   0., -inf, -inf, -inf],\n",
    "        #  [0.,   0.,   0., -inf, -inf],\n",
    "        #  [0.,   0.,   0.,   0., -inf],\n",
    "        #  [0.,   0.,   0.,   0.,   0.]]\n",
    "        \n",
    "        return mask\n",
    "    \n",
    "    def create_pad_mask(self, matrix: torch.tensor, pad_token: int) -> torch.tensor:\n",
    "        # If matrix = [1,2,3,0,0,0] where pad_token=0, the result mask is\n",
    "        # [False, False, False, True, True, True]\n",
    "        return (matrix == pad_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify_data(data, batch_size=16, padding=False, padding_token=-1):\n",
    "    batches = []\n",
    "    for idx in range(0, len(data), batch_size):\n",
    "        # We make sure we dont get the last bit if its not batch_size size\n",
    "        if idx + batch_size < len(data):\n",
    "            # Here you would need to get the max length of the batch,\n",
    "            # and normalize the length with the PAD token.\n",
    "            if padding:\n",
    "                max_batch_length = 0\n",
    "\n",
    "                # Get longest sentence in batch\n",
    "                for seq in data[idx : idx + batch_size]:\n",
    "                    if len(seq) > max_batch_length:\n",
    "                        max_batch_length = len(seq)\n",
    "\n",
    "                # Append X padding tokens until it reaches the max length\n",
    "                for seq_idx in range(batch_size):\n",
    "                    remaining_length = max_bath_length - len(data[idx + seq_idx])\n",
    "                    data[idx + seq_idx] += [padding_token] * remaining_length\n",
    "\n",
    "            batches.append(np.array(data[idx : idx + batch_size]).astype(np.int64))\n",
    "\n",
    "    print(f\"{len(batches)} batches of size {batch_size}\")\n",
    "\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize training and testing data (TODO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = Transformer(\n",
    "    num_tokens=4, dim_model=8, num_heads=2, num_encoder_layers=3, num_decoder_layers=3, dropout_p=0.1\n",
    ").to(device)\n",
    "opt = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model, opt, loss_fn, dataloader):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        X, y = batch[:, 0], batch[:, 1]\n",
    "        X, y = torch.tensor(X).to(device), torch.tensor(y).to(device)\n",
    "\n",
    "        # Now we shift the tgt by one so with the <SOS> we predict the token at pos 1\n",
    "        y_input = y[:,:-1]\n",
    "        y_expected = y[:,1:]\n",
    "        \n",
    "        # Get mask to mask out the next words\n",
    "        sequence_length = y_input.size(1)\n",
    "        tgt_mask = model.get_tgt_mask(sequence_length).to(device)\n",
    "\n",
    "        # Standard training except we pass in y_input and tgt_mask\n",
    "        pred = model(X, y_input, tgt_mask)\n",
    "\n",
    "        # Permute pred to have batch size first again\n",
    "        pred = pred.permute(1, 2, 0)      \n",
    "        loss = loss_fn(pred, y_expected)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    \n",
    "        total_loss += loss.detach().item()\n",
    "        \n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_loop(model, loss_fn, dataloader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            X, y = batch[:, 0], batch[:, 1]\n",
    "            X, y = torch.tensor(X, dtype=torch.long, device=device), torch.tensor(y, dtype=torch.long, device=device)\n",
    "\n",
    "            # Now we shift the tgt by one so with the <SOS> we predict the token at pos 1\n",
    "            y_input = y[:,:-1]\n",
    "            y_expected = y[:,1:]\n",
    "            \n",
    "            # Get mask to mask out the next words\n",
    "            sequence_length = y_input.size(1)\n",
    "            tgt_mask = model.get_tgt_mask(sequence_length).to(device)\n",
    "\n",
    "            # Standard training except we pass in y_input and src_mask\n",
    "            pred = model(X, y_input, tgt_mask)\n",
    "\n",
    "            # Permute pred to have batch size first again\n",
    "            pred = pred.permute(1, 2, 0)      \n",
    "            loss = loss_fn(pred, y_expected)\n",
    "            total_loss += loss.detach().item()\n",
    "        \n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, opt, loss_fn, train_dataloader, val_dataloader, epochs):\n",
    "    # Used for plotting later on\n",
    "    train_loss_list, validation_loss_list = [], []\n",
    "    \n",
    "    print(\"Training and validating model\")\n",
    "    for epoch in range(epochs):\n",
    "        print(\"-\"*25, f\"Epoch {epoch + 1}\",\"-\"*25)\n",
    "        \n",
    "        train_loss = train_loop(model, opt, loss_fn, train_dataloader)\n",
    "        train_loss_list += [train_loss]\n",
    "        \n",
    "        validation_loss = validation_loop(model, loss_fn, val_dataloader)\n",
    "        validation_loss_list += [validation_loss]\n",
    "        \n",
    "        print(f\"Training loss: {train_loss:.4f}\")\n",
    "        print(f\"Validation loss: {validation_loss:.4f}\")\n",
    "        print()\n",
    "        \n",
    "    return train_loss_list, validation_loss_list\n",
    "    \n",
    "# train_loss_list, validation_loss_list = fit(model, opt, loss_fn, train_dataloader, val_dataloader, 10) # Uncomment after initializing training and validation dataloaders"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fd31074e1e62abc321640522a97f7389e0ea94ed14e52d38d4a0bbc4f52db25b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
