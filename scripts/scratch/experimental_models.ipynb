{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\barun\\Miniconda3\\envs\\torch-zoo\\lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: [WinError 127] The specified procedure could not be found\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\barun\\Miniconda3\\envs\\torch-zoo\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3460, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\barun\\AppData\\Local\\Temp\\ipykernel_2784\\1451234061.py\", line 17, in <module>\n",
      "    import torchvision\n",
      "  File \"c:\\Users\\barun\\Miniconda3\\envs\\torch-zoo\\lib\\site-packages\\torchvision\\__init__.py\", line 5, in <module>\n",
      "    from torchvision import datasets\n",
      "  File \"c:\\Users\\barun\\Miniconda3\\envs\\torch-zoo\\lib\\site-packages\\torchvision\\datasets\\__init__.py\", line 1, in <module>\n",
      "    from ._optical_flow import KittiFlow, Sintel, FlyingChairs, FlyingThings3D, HD1K\n",
      "  File \"c:\\Users\\barun\\Miniconda3\\envs\\torch-zoo\\lib\\site-packages\\torchvision\\datasets\\_optical_flow.py\", line 13, in <module>\n",
      "    from .utils import verify_str_arg\n",
      "ImportError: cannot import name 'verify_str_arg' from 'torchvision.datasets.utils' (c:\\Users\\barun\\Miniconda3\\envs\\torch-zoo\\lib\\site-packages\\torchvision\\datasets\\utils.py)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\barun\\Miniconda3\\envs\\torch-zoo\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2057, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"c:\\Users\\barun\\Miniconda3\\envs\\torch-zoo\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1118, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"c:\\Users\\barun\\Miniconda3\\envs\\torch-zoo\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1012, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"c:\\Users\\barun\\Miniconda3\\envs\\torch-zoo\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 865, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"c:\\Users\\barun\\Miniconda3\\envs\\torch-zoo\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 818, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(r))\n",
      "  File \"c:\\Users\\barun\\Miniconda3\\envs\\torch-zoo\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 736, in format_record\n",
      "    result += ''.join(_format_traceback_lines(frame_info.lines, Colors, self.has_colors, lvals))\n",
      "  File \"c:\\Users\\barun\\Miniconda3\\envs\\torch-zoo\\lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"c:\\Users\\barun\\Miniconda3\\envs\\torch-zoo\\lib\\site-packages\\stack_data\\core.py\", line 698, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"c:\\Users\\barun\\Miniconda3\\envs\\torch-zoo\\lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"c:\\Users\\barun\\Miniconda3\\envs\\torch-zoo\\lib\\site-packages\\stack_data\\core.py\", line 649, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "  File \"c:\\Users\\barun\\Miniconda3\\envs\\torch-zoo\\lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"c:\\Users\\barun\\Miniconda3\\envs\\torch-zoo\\lib\\site-packages\\stack_data\\core.py\", line 628, in executing_piece\n",
      "    return only(\n",
      "  File \"c:\\Users\\barun\\Miniconda3\\envs\\torch-zoo\\lib\\site-packages\\executing\\executing.py\", line 164, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import pickle\n",
    "from typing import Any, Callable, List, Optional, Tuple\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Helper functions (taken from CV Project 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_img(img: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Bring image values to [0,1] range\n",
    "    Args:\n",
    "        img: (H,W,C) or (H,W) image\n",
    "    \"\"\"\n",
    "    img -= img.min()\n",
    "    img /= img.max()\n",
    "    return img\n",
    "\n",
    "\n",
    "# def verify(function: Callable) -> str:\n",
    "#     \"\"\"Will indicate with a print statement whether assertions passed or failed\n",
    "#     within function argument call.\n",
    "#     Args:\n",
    "#         function: Python function object\n",
    "#     Returns:\n",
    "#         string that is colored red or green when printed, indicating success\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         function()\n",
    "#         return '\\x1b[32m\"Correct\"\\x1b[0m'\n",
    "#     except AssertionError:\n",
    "#         return '\\x1b[31m\"Wrong\"\\x1b[0m'\n",
    "\n",
    "\n",
    "def rgb2gray(img: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Use the coefficients used in OpenCV, found here:\n",
    "    https://docs.opencv.org/3.4/de/d25/imgproc_color_conversions.html\n",
    "    Args:\n",
    "        Numpy array of shape (M,N,3) representing RGB image in HWC format\n",
    "    Returns:\n",
    "        Numpy array of shape (M,N) representing grayscale image\n",
    "    \"\"\"\n",
    "    # Grayscale coefficients\n",
    "    c = [0.299, 0.587, 0.114]\n",
    "    return img[:, :, 0] * c[0] + img[:, :, 1] * c[1] + img[:, :, 2] * c[2]\n",
    "\n",
    "\n",
    "def PIL_resize(img: np.ndarray, size: Tuple[int, int]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        img: Array representing an image\n",
    "        size: Tuple representing new desired (width, height)\n",
    "    Returns:\n",
    "        img\n",
    "    \"\"\"\n",
    "    img = numpy_arr_to_PIL_image(img, scale_to_255=True)\n",
    "    img = img.resize(size)\n",
    "    img = PIL_image_to_numpy_arr(img)\n",
    "    return img\n",
    "\n",
    "\n",
    "def PIL_image_to_numpy_arr(img: Image, downscale_by_255: bool = True) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        img: PIL Image\n",
    "        downscale_by_255: whether to divide uint8 values by 255 to normalize\n",
    "        values to range [0,1]\n",
    "    Returns:\n",
    "        img\n",
    "    \"\"\"\n",
    "    img = np.asarray(img)\n",
    "    img = img.astype(np.float32)\n",
    "    if downscale_by_255:\n",
    "        img /= 255\n",
    "    return img\n",
    "\n",
    "\n",
    "def im2single(im: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        img: uint8 array of shape (m,n,c) or (m,n) and in range [0,255]\n",
    "    Returns:\n",
    "        im: float or double array of identical shape and in range [0,1]\n",
    "    \"\"\"\n",
    "    im = im.astype(np.float32) / 255\n",
    "    return im\n",
    "\n",
    "\n",
    "def single2im(im: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        im: float or double array of shape (m,n,c) or (m,n) and in range [0,1]\n",
    "    Returns:\n",
    "        im: uint8 array of identical shape and in range [0,255]\n",
    "    \"\"\"\n",
    "    im *= 255\n",
    "    im = im.astype(np.uint8)\n",
    "    return im\n",
    "\n",
    "\n",
    "def numpy_arr_to_PIL_image(img: np.ndarray, scale_to_255: False) -> PIL.Image:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        img: in [0,1]\n",
    "    Returns:\n",
    "        img in [0,255]\n",
    "    \"\"\"\n",
    "    if scale_to_255:\n",
    "        img *= 255\n",
    "    return PIL.Image.fromarray(np.uint8(img))\n",
    "\n",
    "\n",
    "def load_image(path: str) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        path: string representing a file path to an image\n",
    "    Returns:\n",
    "        float_img_rgb: float or double array of shape (m,n,c) or (m,n)\n",
    "           and in range [0,1], representing an RGB image\n",
    "    \"\"\"\n",
    "    img = PIL.Image.open(path)\n",
    "    img = np.asarray(img, dtype=float)\n",
    "    float_img_rgb = im2single(img)\n",
    "    return float_img_rgb\n",
    "\n",
    "\n",
    "def save_image(path: str, im: np.ndarray) -> None:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        path: string representing a file path to an image\n",
    "        img: numpy array\n",
    "    \"\"\"\n",
    "    folder_path = os.path.split(path)[0]\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "    img = copy.deepcopy(im)\n",
    "    img = single2im(img)\n",
    "    pil_img = numpy_arr_to_PIL_image(img, scale_to_255=False)\n",
    "    pil_img.save(path)\n",
    "\n",
    "\n",
    "def hstack_images(img1, img2):\n",
    "    \"\"\"\n",
    "    Stacks 2 images side-by-side and creates one combined image.\n",
    "    Args:\n",
    "    - imgA: A numpy array of shape (M,N,3) representing rgb image\n",
    "    - imgB: A numpy array of shape (D,E,3) representing rgb image\n",
    "    Returns:\n",
    "    - newImg: A numpy array of shape (max(M,D), N+E, 3)\n",
    "    \"\"\"\n",
    "\n",
    "    # CHANGED\n",
    "    imgA = np.array(img1)\n",
    "    imgB = np.array(img2)\n",
    "    Height = max(imgA.shape[0], imgB.shape[0])\n",
    "    Width = imgA.shape[1] + imgB.shape[1]\n",
    "\n",
    "    newImg = np.zeros((Height, Width, 3), dtype=imgA.dtype)\n",
    "    newImg[: imgA.shape[0], : imgA.shape[1], :] = imgA\n",
    "    newImg[: imgB.shape[0], imgA.shape[1] :, :] = imgB\n",
    "\n",
    "    # newImg = PIL.Image.fromarray(np.uint8(newImg))\n",
    "    return newImg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing frames\n",
    "import PIL\n",
    "import numpy as np\n",
    "p_img = PIL.Image.open()\n",
    "def PIL_image_to_numpy_arr(img: Image, downscale_by_255: bool = True) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        img: PIL Image\n",
    "        downscale_by_255: whether to divide uint8 values by 255 to normalize\n",
    "        values to range [0,1]\n",
    "    Returns:\n",
    "        img\n",
    "    \"\"\"\n",
    "    img = np.asarray(img)\n",
    "    img = img.astype(np.float32)\n",
    "    if downscale_by_255:\n",
    "        img /= 255\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P02_105.MP4: 1080.0 x 1920.0\n",
      "P02_102.MP4: 1080.0 x 1920.0\n",
      "P02_137.MP4: 1080.0 x 1920.0\n",
      "P02_130.MP4: 1080.0 x 1920.0\n",
      "P02_121.MP4: 1080.0 x 1920.0\n",
      "P02_126.MP4: 1080.0 x 1920.0\n",
      "P02_128.MP4: 1080.0 x 1920.0\n",
      "P02_113.MP4: 1080.0 x 1920.0\n",
      "P02_114.MP4: 1080.0 x 1920.0\n",
      "P02_131.MP4: 1080.0 x 1920.0\n",
      "P02_136.MP4: 1080.0 x 1920.0\n",
      "P02_103.MP4: 1080.0 x 1920.0\n",
      "P02_104.MP4: 1080.0 x 1920.0\n",
      "P02_115.MP4: 1080.0 x 1920.0\n",
      "P02_112.MP4: 1080.0 x 1920.0\n",
      "P02_129.MP4: 1080.0 x 1920.0\n",
      "P02_127.MP4: 1080.0 x 1920.0\n",
      "P02_120.MP4: 1080.0 x 1920.0\n",
      "P02_111.MP4: 1080.0 x 1920.0\n",
      "P02_116.MP4: 1080.0 x 1920.0\n",
      "P02_118.MP4: 1080.0 x 1920.0\n",
      "P02_123.MP4: 1080.0 x 1920.0\n",
      "P02_124.MP4: 1080.0 x 1920.0\n",
      "P02_135.MP4: 1080.0 x 1920.0\n",
      "P02_132.MP4: 1080.0 x 1920.0\n",
      "P02_109.MP4: 1080.0 x 1920.0\n",
      "P02_107.MP4: 1080.0 x 1920.0\n",
      "P02_125.MP4: 1080.0 x 1920.0\n",
      "P02_122.MP4: 1080.0 x 1920.0\n",
      "P02_119.MP4: 1080.0 x 1920.0\n",
      "P02_117.MP4: 1080.0 x 1920.0\n",
      "P02_110.MP4: 1080.0 x 1920.0\n",
      "P02_101.MP4: 1080.0 x 1920.0\n",
      "P02_106.MP4: 1080.0 x 1920.0\n",
      "P02_108.MP4: 1080.0 x 1920.0\n",
      "P02_133.MP4: 1080.0 x 1920.0\n",
      "P02_134.MP4: 1080.0 x 1920.0\n"
     ]
    }
   ],
   "source": [
    "file_path = \"2g1n6qdydwa9u22shpxqzp0t8m/P02/videos/\"  # change to your own video path\n",
    "files = os.listdir(file_path)\n",
    "for f in files:\n",
    "    path = file_path + f\n",
    "    vid = cv2.VideoCapture(path)\n",
    "    height = vid.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "    width = vid.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
    "    print(f'{f}: {height} x {width}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Narration embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\barun\\Miniconda3\\envs\\torch-zoo\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading (…)e9125/.gitattributes: 100%|██████████| 1.18k/1.18k [00:00<00:00, 294kB/s]\n",
      "Downloading (…)_Pooling/config.json: 100%|██████████| 190/190 [00:00<00:00, 25.3kB/s]\n",
      "Downloading (…)7e55de9125/README.md: 100%|██████████| 10.6k/10.6k [00:00<00:00, 1.93MB/s]\n",
      "Downloading (…)55de9125/config.json: 100%|██████████| 612/612 [00:00<00:00, 122kB/s]\n",
      "Downloading (…)ce_transformers.json: 100%|██████████| 116/116 [00:00<00:00, 21.1kB/s]\n",
      "Downloading (…)125/data_config.json: 100%|██████████| 39.3k/39.3k [00:00<00:00, 727kB/s]\n",
      "Downloading (…)\"pytorch_model.bin\";: 100%|██████████| 90.9M/90.9M [00:10<00:00, 9.04MB/s]\n",
      "Downloading (…)nce_bert_config.json: 100%|██████████| 53.0/53.0 [00:00<00:00, 11.8kB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 112/112 [00:00<00:00, 22.4kB/s]\n",
      "Downloading (…)e9125/tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 3.19MB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 350/350 [00:00<00:00, 70.0kB/s]\n",
      "Downloading (…)9125/train_script.py: 100%|██████████| 13.2k/13.2k [00:00<00:00, 470kB/s]\n",
      "Downloading (…)7e55de9125/vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 2.18MB/s]\n",
      "Downloading (…)5de9125/modules.json: 100%|██████████| 349/349 [00:00<00:00, 69.9kB/s]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2', device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding shape = (384,)\n",
      "embedding = [-2.05573719e-02  5.83466925e-02 -3.61822210e-02 -3.85916838e-03\n",
      " -1.02967501e-01 -2.12387964e-02  1.13360612e-02  2.05004215e-02\n",
      " -1.21941715e-02 -5.22203930e-03  1.22582912e-01 -5.20467684e-02\n",
      " -8.58942643e-02  4.99771610e-02 -5.25188819e-02 -1.00123577e-01\n",
      "  5.90349585e-02  1.52453454e-02  4.67252871e-03  3.19586881e-02\n",
      " -6.30533546e-02  9.30613652e-03  2.04242896e-02  5.70746977e-03\n",
      " -2.00125296e-02  8.88943598e-02 -1.95298449e-03  6.55974597e-02\n",
      " -2.26503033e-02 -1.94702987e-02 -1.39277820e-02 -5.86143062e-02\n",
      " -1.85706150e-02  9.50346701e-03 -1.17636397e-02  5.33422902e-02\n",
      " -3.14691029e-02  7.37754852e-02 -3.37130278e-02 -2.92711724e-02\n",
      " -3.96612473e-02  1.60753317e-02  4.74423803e-02 -2.67440602e-02\n",
      "  8.83961916e-02  4.18313444e-02 -1.40461382e-02  2.41288580e-02\n",
      "  9.60046723e-02 -3.51824276e-02 -2.10427269e-02  8.50031618e-03\n",
      " -2.69936658e-02 -8.40251446e-02  3.78844957e-03  5.67627512e-02\n",
      "  2.86040478e-03  6.23897761e-02  4.86243144e-03  3.77090648e-02\n",
      "  5.61857224e-02 -2.51113740e-03 -5.27414307e-02  1.09111173e-02\n",
      " -5.68423327e-03 -2.94399858e-02  3.14119644e-02 -3.51963229e-02\n",
      " -1.21385725e-02  5.83947711e-02  1.04132392e-01 -8.59086122e-03\n",
      " -3.32995914e-02 -3.65892164e-02  4.57366221e-02 -3.37202512e-02\n",
      "  3.69035490e-02 -1.31917438e-02 -7.39422720e-03  5.97440042e-02\n",
      " -8.63396004e-02  5.01945987e-02 -4.37065819e-03  8.25413689e-02\n",
      " -2.16894206e-02 -9.60319361e-04 -4.46637012e-02  2.88252924e-02\n",
      " -4.11921628e-02  2.11008769e-02  5.64930029e-02  6.33121468e-03\n",
      "  2.22904179e-02  4.88890000e-02 -2.71789953e-02 -3.71125783e-03\n",
      " -4.69169877e-02  2.90290229e-02 -6.73516318e-02  8.03011283e-02\n",
      "  1.50124554e-03 -3.67771536e-02 -5.47415242e-02  5.74848093e-02\n",
      "  1.48336310e-02  3.98188308e-02  3.74417230e-02  1.10421404e-02\n",
      " -7.40920706e-03 -6.75112605e-02  1.23758838e-02  1.16102919e-01\n",
      " -8.05766359e-02  1.84167866e-02  3.78200505e-03 -5.77178448e-02\n",
      "  1.43785393e-02 -7.68760312e-03 -1.28434394e-02 -1.51711162e-02\n",
      " -2.32950039e-02  4.69124056e-02 -3.94091085e-02  2.14739572e-02\n",
      " -4.06720378e-02 -4.30867076e-02  7.63741434e-02 -7.59946307e-34\n",
      "  1.59690008e-02  4.74745706e-02 -1.28739513e-02 -3.65998298e-02\n",
      "  2.52671055e-02 -1.48745757e-02  3.20562124e-02  3.94064337e-02\n",
      " -3.68333259e-03  5.74109331e-02  6.41624704e-02 -1.25240222e-01\n",
      " -4.77121919e-02 -1.28973080e-02  1.13064218e-02 -6.54694587e-02\n",
      "  2.72655319e-02  5.95409311e-02 -1.02512073e-02 -1.82835124e-02\n",
      "  3.96309868e-02 -5.24440445e-02  1.64273437e-02  1.30389715e-02\n",
      "  1.48920808e-02  1.01348668e-01  4.47389558e-02 -1.08435549e-01\n",
      " -2.99150986e-03  3.25496644e-02 -2.37346217e-02  2.64745690e-02\n",
      "  6.21562041e-02 -5.99558502e-02 -8.33428353e-02  7.72501379e-02\n",
      " -3.90234701e-02 -1.45618143e-02 -5.12269773e-02 -5.64412549e-02\n",
      "  4.20339033e-02  7.35333487e-02 -1.32353559e-01  1.29473908e-02\n",
      "  5.78358807e-02 -6.57243356e-02  5.93155697e-02  7.56938830e-02\n",
      " -7.73224700e-03 -6.02287101e-03  2.81170737e-02 -2.98925918e-02\n",
      "  1.39839932e-01  1.70650315e-02 -1.21502936e-01  2.09643599e-02\n",
      "  2.79560741e-02  5.42972870e-02 -1.46689424e-02 -1.22195098e-03\n",
      "  7.30316192e-02  5.45420013e-02 -1.97599549e-02  1.08714245e-01\n",
      " -3.33114862e-02 -1.01992432e-02  2.12936625e-02 -3.32103409e-02\n",
      " -7.24730222e-03 -6.39490187e-02 -5.05548492e-02 -8.20129439e-02\n",
      " -7.47130439e-03  4.07407396e-02 -1.07815601e-01  3.46369371e-02\n",
      " -2.51374226e-02  1.01300399e-03 -6.43282831e-02 -7.10093156e-02\n",
      " -3.62501740e-02  1.60610918e-02 -3.18468525e-03 -3.11840847e-02\n",
      "  2.92653330e-02  4.06251848e-02  6.61704615e-02 -3.91742103e-02\n",
      "  1.04963146e-01  1.44028608e-02 -9.15030092e-02 -1.01110183e-01\n",
      "  2.40404941e-02 -4.86850068e-02  3.17338333e-02  8.58601094e-34\n",
      "  8.32995865e-03  2.02219840e-02  1.58541780e-02 -1.88859738e-02\n",
      " -1.61230620e-02 -3.13203707e-02 -9.42983478e-03  2.80065816e-02\n",
      " -6.90304562e-02  2.18694890e-03 -9.54650864e-02 -1.99823659e-02\n",
      " -2.39817481e-02 -4.74766418e-02  3.57606150e-02  3.05436086e-02\n",
      "  6.30235393e-03  4.52461876e-02 -7.09448904e-02  2.65845452e-02\n",
      " -4.71016131e-02  1.26955630e-02  1.02392539e-01  4.32671122e-02\n",
      " -7.05394894e-03  2.14866269e-02  3.71672772e-02  3.92108634e-02\n",
      " -4.97700982e-02 -4.09110747e-02 -7.06475228e-02 -7.95041695e-02\n",
      "  3.33841518e-02  3.41344923e-02 -8.39342773e-02  1.23542793e-01\n",
      " -6.40724525e-02  3.60649042e-02 -1.77885350e-02  1.00928661e-03\n",
      " -3.48484218e-02  6.22074232e-02 -3.34592350e-02  1.13469198e-01\n",
      " -9.16355103e-02 -7.03525555e-04  6.87980950e-02 -2.18586139e-02\n",
      " -3.17253056e-03  1.07489496e-01  2.27675773e-03  4.75358916e-03\n",
      "  2.66595241e-02 -1.44764408e-02  1.86573546e-02 -2.32649851e-03\n",
      "  5.31681664e-02 -9.20326337e-02 -5.43454923e-02 -5.63440323e-02\n",
      " -1.86069135e-03  3.08902804e-02  1.17519340e-02  1.07024096e-01\n",
      "  1.48863971e-01 -8.46044719e-02 -1.27119785e-02 -4.23051044e-03\n",
      " -7.21643940e-02  1.56678054e-02 -9.95822549e-02  9.22306478e-02\n",
      "  9.58359912e-02  1.35908751e-02  3.14170658e-03 -2.86140125e-02\n",
      " -3.17554958e-02 -4.43307497e-02 -2.22526956e-02  2.86843535e-03\n",
      " -1.29928393e-02 -1.12027109e-01 -9.03280918e-03  5.05539440e-02\n",
      "  8.28783289e-02 -5.08299507e-02  6.20822655e-03  4.36508246e-02\n",
      " -7.19830021e-02 -4.80283139e-04 -4.78957556e-02  1.07070096e-02\n",
      "  1.44623056e-01  2.75017917e-02  4.22758311e-02 -1.44627670e-08\n",
      "  5.96295930e-02  3.84771591e-03 -1.99454874e-02  1.43623166e-02\n",
      " -3.21531445e-02  4.52718651e-03 -6.51193932e-02 -4.75096591e-02\n",
      " -1.28694586e-02 -1.00945316e-01  3.47459479e-03 -1.31913358e-02\n",
      "  1.33924345e-02  3.22423540e-02 -1.19653575e-01  9.85964574e-03\n",
      "  1.26239229e-02 -1.59617718e-02 -1.79729965e-02 -6.58238232e-02\n",
      " -3.13526504e-02 -2.22851224e-02  1.04387194e-01 -4.09735600e-03\n",
      "  1.98770519e-02  1.42561775e-02 -3.52827460e-02  5.52018806e-02\n",
      "  2.10513175e-02  5.98157272e-02  2.21791919e-02 -5.03811911e-02\n",
      "  1.12855852e-01  4.46471088e-02  9.40803885e-02  1.93208188e-03\n",
      " -6.25533015e-02  1.90196745e-02  9.16193984e-03 -6.85694255e-03\n",
      " -7.56069086e-03 -1.46589037e-02 -6.48247674e-02  5.13796359e-02\n",
      " -7.78417066e-02 -1.22597311e-02  3.28184888e-02  2.69824564e-02\n",
      "  4.75083914e-04 -1.34381782e-02  4.79360595e-02 -4.82159033e-02\n",
      " -3.79633671e-03 -2.92603001e-02  2.89747771e-02  9.76091400e-02\n",
      "  5.52766584e-02 -4.83577289e-02  3.51479575e-02  3.19772325e-02\n",
      " -7.24112391e-02 -7.51481252e-03 -6.74696341e-02 -4.13919427e-02]\n"
     ]
    }
   ],
   "source": [
    "sentence = 'take knife and put back plate'\n",
    "embedding = model.encode(sentence)\n",
    "print(f'embedding shape = {embedding.shape}')\n",
    "print(f'embedding = {embedding}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: https://towardsdatascience.com/3-types-of-contextualized-word-embeddings-from-bert-using-transfer-learning-81fcefe3fe6d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|██████████| 570/570 [00:00<00:00, 104kB/s]\n",
      "c:\\Users\\barun\\Miniconda3\\envs\\torch-zoo\\lib\\site-packages\\huggingface_hub\\file_download.py:129: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\barun\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading (…)\"pytorch_model.bin\";: 100%|██████████| 440M/440M [00:57<00:00, 7.69MB/s] \n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Downloading (…)solve/main/vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 2.57MB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 28.0/28.0 [00:00<00:00, 6.99kB/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states=True)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_text_preparation(text, tokenizer):\n",
    "    marked_text = \"[CLS]\" + text + \"[SEP]\"\n",
    "    tokenized_text = tokenizer.tokenize(marked_text)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    segments_ids = [1]*len(indexed_tokens)\n",
    "\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensor = torch.tensor([segments_ids])\n",
    "\n",
    "    return tokenized_text, tokens_tensor, segments_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_embeddings(tokens_tensor, segments_tensor, model):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens_tensor, segments_tensor)\n",
    "        hidden_states = outputs[2][1:]\n",
    "\n",
    "    token_embeddings = hidden_states[-1].squeeze()\n",
    "    list_token_embeddings = [token_embed.tolist() for token_embed in token_embeddings]\n",
    "\n",
    "    return list_token_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = ['tap', 'tap:water', 'water:tap', 'flow:tap', 'tab', 'tap:hot']\n",
    "word = 'tap'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'tap' is not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m tokenized_text, tokens_tensor, segments_tensor \u001b[39m=\u001b[39m bert_text_preparation(text, tokenizer)\n\u001b[0;32m      4\u001b[0m list_token_embeddings \u001b[39m=\u001b[39m get_bert_embeddings(tokens_tensor, segments_tensor, model)\n\u001b[1;32m----> 5\u001b[0m word_index \u001b[39m=\u001b[39m tokenized_text\u001b[39m.\u001b[39;49mindex(word)\n\u001b[0;32m      6\u001b[0m word_embedding \u001b[39m=\u001b[39m list_token_embeddings[word_index]\n\u001b[0;32m      8\u001b[0m target_word_embeddings\u001b[39m.\u001b[39mappend(word_embedding)\n",
      "\u001b[1;31mValueError\u001b[0m: 'tap' is not in list"
     ]
    }
   ],
   "source": [
    "target_word_embeddings = []\n",
    "for text in texts:\n",
    "    tokenized_text, tokens_tensor, segments_tensor = bert_text_preparation(text, tokenizer)\n",
    "    list_token_embeddings = get_bert_embeddings(tokens_tensor, segments_tensor, model)\n",
    "    word_index = tokenized_text.index(word)\n",
    "    word_embedding = list_token_embeddings[word_index]\n",
    "\n",
    "    target_word_embeddings.append(word_embedding)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcualte distance between the word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "list_of_distances = []\n",
    "for text1, embed1 in zip(texts, target_word_embeddings):\n",
    "    for text2, embed2 in zip(texts, target_word_embeddings):\n",
    "        cos_dist = 1 - cosine(embed1, embed2)\n",
    "        list_of_distances.append([text1, text2, cos_dist])\n",
    "\n",
    "distances_df = pd.DataFrame(list_of_distances, \n",
    "                            columns=['text1', 'text2', 'cosine_distance'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-zoo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bb3c50f69d774e00a3679a8b915ce4d62999e56994354393d792a69337588e8f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
